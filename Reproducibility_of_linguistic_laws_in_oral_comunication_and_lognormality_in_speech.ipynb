{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import scipy.stats\n",
    "import more_itertools as mit\n",
    "import urllib\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histograma(dataset, numero_bins=50, type_bins='log'):\n",
    "    \"\"\"\n",
    "    Calcula el histograma de una serie de datos y guarda el resultado\n",
    "    El input es una lista, no sirve meter como input un histograma X-Y\n",
    "    INPUT:\n",
    "        dataset (list, array): Datos sobre los que se hacen el histograma\n",
    "        header (str): String for using in header and in file name\n",
    "        output_path (str): Donde se guardara el resultado. IF OUTPUT_PATH=FALSE\n",
    "                            devuelve el histograma y no lo guarda a archivo\n",
    "        numero_bins (int): Numero de bins que se realizaran\n",
    "        type_bins ('log', 'lin'): Relacion para generar los bins\n",
    "        output_name (str): Name for the output file with extension .txt\n",
    "        header_adicional (str): Aditional header info only for the file header\n",
    "    OUTPUT:\n",
    "        Guarda el resultado en output_path\n",
    "        if output_path == FALSE: Devuelve bin_mean_x, counts_mean\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = np.asarray(dataset)\n",
    "\n",
    "    # Defining edges depending if log or lin\n",
    "    if (type_bins == 'log'):\n",
    "        min_x = np.log10(np.min(dataset[dataset.nonzero()]))\n",
    "        max_x = np.log10(np.max(dataset))\n",
    "        bin_edges = np.logspace(min_x, max_x, numero_bins)\n",
    "        \n",
    "    elif (type_bins == 'lin'):\n",
    "        min_x = np.min(dataset[dataset.nonzero()])\n",
    "        max_x = np.max(dataset)\n",
    "        bin_edges = np.linspace(min_x, max_x, numero_bins)\n",
    "\n",
    "    # En que bin cae cada dato\n",
    "    index = np.digitize(dataset, bin_edges)\n",
    "\n",
    "    # Frecuencia de datos por bin\n",
    "    datosPorBin = [len(dataset[index == i]) for i in range(1, len(bin_edges))]\n",
    "\n",
    "    # Adimensionaliza la frecuencia por la anchura del bin\n",
    "    counts_mean = datosPorBin/np.diff(bin_edges)\n",
    "    counts_mean = counts_mean[counts_mean != 0]\n",
    "    \n",
    "    # version normalizada\n",
    "    masatotal = np.sum(datosPorBin)\n",
    "    counts_norm = counts_mean/masatotal\n",
    "\n",
    "    # Posicion media del eje x\n",
    "    bin_mean_x = [dataset[index == i].mean() for i in range(1, len(bin_edges))\n",
    "                  if len(np.asarray(dataset[index == i])) > 0]\n",
    "\n",
    "    return(bin_mean_x, counts_mean, counts_norm, bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binear_datos(lista_x, lista_y, bins=50, log = True):\n",
    "    \"\"\"\n",
    "    Dado una lista de par de puntos (X,Y) binea dando lugar a una lista equivalente\n",
    "    pero de menor dimension. Sirve para eliminar el ruido, variaciones, etc.\n",
    "    INPUT:\n",
    "        lista_x (list, array): Lista de puntos de igual dimension que lista_y\n",
    "        lista_y (list, array): Lista de puntos de igual dimension que lista_x\n",
    "        bins (int): numero de bins a utilizar\n",
    "        log (boolean): True-> bins logaritmico, False-> bins lineales\n",
    "    OUTPUT:\n",
    "        posiciones (array): Centro de gravedad del bin. lista de posiciones X\n",
    "        frecuencia_media(array): Frecuencia pesada en los bins\n",
    "    \"\"\"\n",
    "    # Definicion de los bins\n",
    "    if log:\n",
    "        bin_edges = np.logspace(np.log10(min(lista_x)), np.log10(max(lista_x)), bins)\n",
    "    else:\n",
    "         bin_edges = np.linspace(min(lista_x), max(lista_x), bins)\n",
    "\n",
    "    diccionario = dict() # valor*posicion\n",
    "    diccionario_frecs = dict()\n",
    "    for indice_x, value_x in enumerate(lista_x):\n",
    "        indice_caja = np.searchsorted(bin_edges, value_x)\n",
    "        if indice_caja in diccionario:\n",
    "            diccionario[indice_caja].append(lista_x[indice_x]*lista_y[indice_x])\n",
    "            diccionario_frecs[indice_caja].append(lista_y[indice_x])\n",
    "        else:\n",
    "            diccionario[indice_caja] = list()\n",
    "            diccionario_frecs[indice_caja] = list()\n",
    "            diccionario[indice_caja].append(lista_x[indice_x]*lista_y[indice_x])\n",
    "            diccionario_frecs[indice_caja].append(lista_y[indice_x])\n",
    "\n",
    "    frecuencia_media = []\n",
    "    for key, value in diccionario_frecs.items():\n",
    "        if len(value) > 0:\n",
    "            frecuencia_media.append(np.mean(value))\n",
    "\n",
    "        elif len(value) == 0:\n",
    "            frecuencia_media.append(0)\n",
    "\n",
    "    posiciones = []\n",
    "    for key, value in diccionario.items():\n",
    "        if len(value) > 0:\n",
    "            posiciones.append(np.sum(value)/np.sum(diccionario_frecs[key]))\n",
    "        elif len(value) == 0:\n",
    "            posiciones.append((bin_edges[key] + bin_edges[key+1])/2)\n",
    "            \n",
    "    ## Ordenamos los datos\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({\"pos\": posiciones, \"freqs\": frecuencia_media})\n",
    "    df = df.sort_values(\"pos\")\n",
    "\n",
    "    posiciones = df.pos.values\n",
    "    frecuencia_media = df.freqs.values\n",
    "    \n",
    "    return posiciones, frecuencia_media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subplot_axes(ax, rect, fig, axisbg='w'):\n",
    "    fig = plt.gcf()\n",
    "    box = ax.get_position()\n",
    "    width = box.width\n",
    "    height = box.height\n",
    "    inax_position  = ax.transAxes.transform(rect[0:2])\n",
    "    transFigure = fig.transFigure.inverted()\n",
    "    infig_position = transFigure.transform(inax_position)    \n",
    "    x = infig_position[0]\n",
    "    y = infig_position[1]\n",
    "    width *= rect[2]\n",
    "    height *= rect[3]  # <= Typo was here\n",
    "    subax = fig.add_axes([x,y,width,height],axisbg=axisbg, xscale = 'log', yscale ='log')\n",
    "    #subax.xaxis.tick_top()\n",
    "    subax.xaxis.set_label_position('bottom') \n",
    "    #subax.yaxis.tick_right()\n",
    "    subax.yaxis.set_label_position('left')\n",
    "    x_labelsize = subax.get_xticklabels()[0].get_size()\n",
    "    y_labelsize = subax.get_yticklabels()[0].get_size()\n",
    "    x_labelsize *= rect[2]**0.5\n",
    "    y_labelsize *= rect[3]**0.5\n",
    "    subax.xaxis.set_tick_params(labelsize=x_labelsize)\n",
    "    subax.yaxis.set_tick_params(labelsize=y_labelsize)\n",
    "    return(subax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcula las figuras de linguistic laws in human comunication\n",
    "REQUISITO: Tener una tabla con las duraciones de cada fonema, palabra, etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processed data\n",
    "#url = 'https://datadryad.org/bitstream/handle/10255/dryad.222397/TablaTranscripcion.csv?sequence=1'  \n",
    "#urllib.request.urlretrieve(url, \"./TablaTranscripcion.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'TablaTranscripcion.csv'\n",
    "\n",
    "# Cargar datos\n",
    "tabla_datos = pd.read_csv(input_path, delimiter=\",\", skiprows=2, \n",
    "                            names=['token', 'tinit', 'tend', 'duration', 'sentence', 'path', 'tipe', 'numtoken', 'numphonemes',\n",
    "                                  'numletters'])\n",
    "# Para recuperar los okeys\n",
    "#tabla_datos[tabla_datos.numtoken.isin(tabla_datos[tabla_datos.token == \"okay\"].numtoken)]\n",
    "tabla_datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos y nos quedamos solo con palabras\n",
    "lista_words = tabla_datos[((tabla_datos.tipe == 'w') & (tabla_datos.token != 'SIL')\n",
    "                           & (tabla_datos.token != 'VOCNOISE') & (tabla_datos.token != 'UNKNOWN') \n",
    "                           & (tabla_datos.token != 'NOISE') & (tabla_datos.token != 'LAUGH'))]\n",
    "\n",
    "# Creamos una columna que indique el numero de caracteres de esa palabra\n",
    "lista_words = lista_words.assign(dur_characteres=lista_words.token.str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos y nos quedamos solo con fonemas\n",
    "lista_fonema = tabla_datos[((tabla_datos.tipe == 'p') & (tabla_datos.token != 'SIL') & (tabla_datos.token != 'VOCNOISE') \n",
    "                            & (tabla_datos.token != 'UNKNOWN') & (tabla_datos.token != 'NOISE') & (tabla_datos.token != 'LAUGH')\n",
    "                            & (tabla_datos.token != 'IVER')\n",
    "                            & (tabla_datos.token.str.endswith(\"}\") == False) & (tabla_datos.token.str.startswith(\"{\") == False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos las duraciones de las frases\n",
    "dur_sentences = tabla_datos[tabla_datos.tipe == \"w\"].groupby(\"sentence\").sum().duration\n",
    "# print(lista_fonema.token.unique())\n",
    "# print(len(lista_fonema.token.unique()))\n",
    "# tabla_datos[tabla_datos.token == \"the\"].head()\n",
    "# tabla_datos[tabla_datos.numtoken == 131]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener las tablas 1 y 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "\n",
    "def imprimir_estadistica(listaduration, rnd, string):\n",
    "    \n",
    "    phonN = len(listaduration)\n",
    "    phonMean = np.round(np.mean(listaduration), rnd)\n",
    "    phonStd = np.round(np.std(listaduration), rnd)\n",
    "    phonMode = mode(np.round(listaduration, rnd))\n",
    "    phonMedian = np.round(np.median(listaduration), rnd)\n",
    "    phonP10 = np.round(np.percentile(listaduration, 10), rnd)\n",
    "    phonP90 = np.round(np.percentile(listaduration, 90), rnd)\n",
    "    print(string + str(phonN) +\"  \"+ str(phonMean) + \"  \" + str(phonStd) +\n",
    "      \"  \" + str(phonMode) + \"  \" + str(phonMedian) + \"  \" + str(phonP10) +\n",
    "     \"  \" + str(phonP90))\n",
    "\n",
    "print(\"                    Time Duration (s)\")\n",
    "print(\"           n     Mean  Std  Mode  Median  p10  p90\")\n",
    "\n",
    "imprimir_estadistica(lista_fonema.duration, 2, \"Phoneme:   \")\n",
    "imprimir_estadistica(lista_words.duration, 2, \"Word:      \")\n",
    "imprimir_estadistica(dur_sentences, 1, \"BG:          \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imprimir_estadistica(listaduration, rnd, string):\n",
    "    \n",
    "    phonMean = np.round(np.mean(listaduration), rnd)\n",
    "    phonStd = np.round(np.std(listaduration), rnd)\n",
    "    phonMode = mode(np.round(listaduration, rnd))\n",
    "    phonMedian = np.round(np.median(listaduration), rnd)\n",
    "    phonP10 = np.round(np.percentile(listaduration, 10), rnd)\n",
    "    phonP90 = np.round(np.percentile(listaduration, 90), rnd)\n",
    "    print(string + str(phonMean) + \"  \" + str(phonStd) +\n",
    "      \"  \" + str(phonMode) + \"  \" + str(phonMedian) + \"  \" + str(phonP10) +\n",
    "     \"  \" + str(phonP90))\n",
    "\n",
    "print(\"                    Num Characters\")\n",
    "print(\"         Mean Std  Mode  Median p10 p90\")\n",
    "\n",
    "numcharperphoneme = lista_words.numletters/lista_words.numphonemes\n",
    "numcharperphoneme = numcharperphoneme[numcharperphoneme<7]\n",
    "\n",
    "imprimir_estadistica(numcharperphoneme, 1, \"Phoneme:   \")\n",
    "imprimir_estadistica(lista_words.numletters, 0, \"Word:      \")\n",
    "char_sentences = tabla_datos[tabla_datos.tipe == \"w\"].groupby(\"sentence\").sum().numletters\n",
    "imprimir_estadistica(char_sentences, 0, \"BG:       \")\n",
    "\n",
    "print(\"\")\n",
    "print(\"                    Num Phonemas\")\n",
    "imprimir_estadistica(lista_words.numphonemes, 1, \"Word:      \")\n",
    "phon_sentences = tabla_datos[tabla_datos.tipe == \"w\"].groupby(\"sentence\").sum().numphonemes\n",
    "imprimir_estadistica(phon_sentences, 0, \"BG:      \")\n",
    "\n",
    "print(\"\")\n",
    "print(\"                    Num Words\")\n",
    "words_sentences = tabla_datos[tabla_datos.tipe == \"w\"].groupby(\"sentence\").count().numphonemes\n",
    "imprimir_estadistica(words_sentences, 0, \"BG:      \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJEMPLO DE LA FRASE MÃ¡s LARGA\n",
    "#a = tabla_datos[tabla_datos.tipe == \"w\"].groupby(\"sentence\").size()\n",
    "#frase = a[a==a.max()].index[0]\n",
    "\n",
    "#tabla_datos[(tabla_datos.sentence == frase) & (tabla_datos.tipe == \"w\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EJEMPLO DE LA FRASE Mas corta\n",
    "# # Obtenemos las duraciones de las frases\n",
    "# frases1palabra = tabla_datos[(tabla_datos.tipe == \"w\")].groupby(\"sentence\").size()\n",
    "# frases1palabra = frases1palabra[frases1palabra==1].index\n",
    "\n",
    "# subset = tabla_datos[tabla_datos['sentence'].isin(np.array(frases1palabra))]\n",
    "# subset[(subset.tipe == \"w\") & (subset.duration < 0.15)]\n",
    "#subset.groupby(\"sentence\").size()\n",
    "#a = .groupby(\"sentence\").size()\n",
    "#a = tabla_datos[(tabla_datos.tipe == \"w\")].groupby(\"sentence\").size()\n",
    "#frase = a[a==1]\n",
    "#frase\n",
    "\n",
    "#tabla_datos[(tabla_datos.sentence == frase) & (tabla_datos.tipe == \"w\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Histograma duraciones fonemas, palabras, frases y colapso\n",
    "\n",
    "Calculamos los histogramas de duraciones en segundos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Definimos adecuadamente el subconjunto de fonemas y palabras\n",
    "fonemes_durationsall = lista_fonema.duration.values[(lista_fonema.duration.values<5) & (lista_fonema.duration.values>0)]\n",
    "words_durationsall = lista_words.duration.values[(lista_words.duration.values<5) & (lista_words.duration.values>0) ]\n",
    "dur_sentencesall = tabla_datos[tabla_datos.tipe == \"w\"].groupby(\"sentence\").sum().duration\n",
    "\n",
    "listainformant = [\"s01/\", \"s02/\", \"s03/\", \"s04/\", \"s05/\", \"s06/\", \"s07/\", \"s08/\", \"s09/\"]\n",
    "\n",
    "for i in range(10):\n",
    "    if i == 0:\n",
    "        fonemes_durations = fonemes_durationsall\n",
    "        words_durations = words_durationsall\n",
    "        dur_sentences = dur_sentencesall\n",
    "\n",
    "    else:\n",
    "        lista_fonema2 = lista_fonema[lista_fonema.path.str.startswith(listainformant[i-1])]\n",
    "        fonemes_durations2 = lista_fonema2.duration.values[(lista_fonema2.duration.values<5) & (lista_fonema2.duration.values>0)]\n",
    "\n",
    "        lista_words2 = lista_words[lista_words.path.str.startswith(listainformant[i-1])]\n",
    "        words_durations2 = lista_words2.duration.values[(lista_words2.duration.values<5) & (lista_words2.duration.values>0) ]\n",
    "\n",
    "        tabla_datos2 = tabla_datos[tabla_datos.path.str.startswith(listainformant[i-1])]\n",
    "        dur_sentences2 = tabla_datos2[tabla_datos2.tipe == \"w\"].groupby(\"sentence\").sum().duration\n",
    "                       \n",
    "        fonemes_durations = fonemes_durations2\n",
    "        words_durations = words_durations2\n",
    "        #dur_sentences = dur_sentences2\n",
    "\n",
    "\n",
    "    # Realizamos los histogramas\n",
    "    bin_mean_x_fon, _, counts_norm_fon, _ = histograma(fonemes_durations, numero_bins=35)\n",
    "    bin_mean_x_word, _, counts_norm_word, _ = histograma(words_durations, numero_bins=20)\n",
    "    bin_mean_x_sentence, _, counts_norm_sentence, _ = histograma(dur_sentences, numero_bins=22)\n",
    "\n",
    "\n",
    "    # Ploteamos\n",
    "    f, ax = plt.subplots()\n",
    "    ax.plot(bin_mean_x_fon, counts_norm_fon, '-s', lw = 2, ms = 7, label= 'phonemes', zorder=1, c=\"darkorange\")\n",
    "    ax.plot(bin_mean_x_word, counts_norm_word, '-o', lw = 2, ms = 7, label= 'words', zorder=2, c='darkblue')\n",
    "    ax.plot(bin_mean_x_sentence, counts_norm_sentence, '-d', lw = 2, ms = 7, label= 'BG', zorder=3, c=\"g\")\n",
    "\n",
    "\n",
    "    ax.set_ylim([-0.3, 11.3])\n",
    "    ax.set_xlim([0, 2])\n",
    "\n",
    "    ax.set_xlabel(\"t (seconds)\", fontsize=14)\n",
    "    ax.set_ylabel(\"P(t)\", fontsize=14)\n",
    "\n",
    "    # Table\n",
    "\n",
    "\n",
    "    col_labels=[r'$<t>$',r'$mode$']\n",
    "    row_labels=['phon','words','senten']\n",
    "    table_vals=[[np.round(np.mean(lista_fonema.duration.values[lista_fonema.duration.values<4]),3),\n",
    "                 scipy.stats.mode(np.round(lista_fonema.duration.values[lista_fonema.duration.values<4], 2))[0][0]],\n",
    "\n",
    "                [np.round(np.mean(lista_words.duration.values[lista_words.duration.values<4]),2),\n",
    "                 scipy.stats.mode(np.round(lista_words.duration.values[lista_words.duration.values<4], 2))[0][0]],\n",
    "\n",
    "               [np.round(np.mean(dur_sentences), 1),\n",
    "                   scipy.stats.mode(np.round(dur_sentences, 2))[0][0]]]\n",
    "\n",
    "\n",
    "    ######################################\n",
    "    ## COLAPSO DE LOS DATOS##############3\n",
    "    # Vamos a intentar colapsar los datos\n",
    "    log_fonemes_dur = np.log(fonemes_durations)\n",
    "    log_fonemes_dur = (log_fonemes_dur - np.mean(log_fonemes_dur))/np.std(log_fonemes_dur)\n",
    "\n",
    "    log_words_dur = np.log(words_durations)\n",
    "    log_words_dur = (log_words_dur - np.mean(log_words_dur))/np.std(log_words_dur)\n",
    "\n",
    "    log_sentences_dur = np.log(dur_sentences)\n",
    "    log_sentences_dur = (log_sentences_dur - np.mean(log_sentences_dur))/np.std(log_sentences_dur)\n",
    "\n",
    "    # Datos Normales 0,1\n",
    "    randomNormal = np.random.normal(0, 1, 10000000)\n",
    "\n",
    "    # Realizamos los histogramas\n",
    "    bin_mean_x_fon_collapsed, _, counts_norm_fon_collapsed, _ = histograma(log_fonemes_dur, numero_bins=35, type_bins=\"lin\")\n",
    "    bin_mean_x_word_collapsed, _, counts_norm_word_collapsed, _ = histograma(log_words_dur, numero_bins=20, type_bins=\"lin\")\n",
    "    bin_mean_x_sentence_collapsed, _, counts_norm_sentence_collapsed, _ = histograma(log_sentences_dur, numero_bins=12, type_bins=\"lin\")\n",
    "\n",
    "    bin_mean_x_normal, _, counts_norm_normal, _ = histograma(randomNormal, numero_bins=200, type_bins=\"lin\")\n",
    "\n",
    "\n",
    "    # Ploteamos\n",
    "    subax = plt.axes([0.55, 0.53, .35, .35])\n",
    "    subax.plot(bin_mean_x_fon_collapsed, counts_norm_fon_collapsed, 's', lw = 2, ms = 7, label= 'phonemes', zorder=1, c=\"darkorange\")\n",
    "    subax.plot(bin_mean_x_word_collapsed, counts_norm_word_collapsed, 'o', lw = 2, ms = 7, label= 'words', zorder=2, c='darkblue')\n",
    "    subax.plot(bin_mean_x_sentence_collapsed, counts_norm_sentence_collapsed, 'd', lw = 2, ms = 7, label= 'BG', zorder=3, c=\"g\")\n",
    "    subax.plot(bin_mean_x_normal, counts_norm_normal, '-s', lw = 1, ms = 0, label= 'phonemes', zorder=5, c = 'k')\n",
    "\n",
    "\n",
    "    subax.set_xlim([-5.1,5.1])\n",
    "    subax.set_ylim([0, 0.43])\n",
    "    subax.set_xticks([-3,0,3])\n",
    "    subax.set_yticks([0, 0.2, 0.4])\n",
    "\n",
    "    subax.set_xlabel(\"t'\", fontsize=14)\n",
    "\n",
    "\n",
    "    subax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "    ax.set_xticks([0, 0.5, 1, 1.5, 2])\n",
    "    ax.set_yticks([0, 4, 8])\n",
    "\n",
    "    ax.legend(loc=(0.11, 0.65), frameon=False, fontsize = 12)\n",
    "\n",
    "\n",
    "    if i == 0:\n",
    "        f.savefig(\"1_Probability_distribution_duration.pdf\")\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_ylim([0.001, 13])\n",
    "        ax.set_xlim([0, 5])\n",
    "        ax.set_xticks([0, 2, 4])\n",
    "\n",
    "        f.savefig(\"SI_LOG_1_Probability_distribution_duration.pdf\")\n",
    "\n",
    "    else:        \n",
    "        f.savefig(\"1_Probability_distribution_duration_informant\"+str(i)+\".pdf\", bbox_inches='tight')\n",
    "\n",
    "        \n",
    "##########################################\n",
    "# Realizamos los histogramas\n",
    "fonemes_durations = lista_fonema.duration.values[(lista_fonema.duration.values<5) & (lista_fonema.duration.values>0)]\n",
    "words_durations = lista_words.duration.values[(lista_words.duration.values<5) & (lista_words.duration.values>0) ]\n",
    "dur_sentences = tabla_datos[tabla_datos.tipe == \"w\"].groupby(\"sentence\").sum().duration\n",
    "\n",
    "bin_mean_x_fon, _, counts_norm_fon, _ = histograma(fonemes_durationsall, numero_bins=35)\n",
    "bin_mean_x_word, _, counts_norm_word, _ = histograma(words_durationsall, numero_bins=20)\n",
    "bin_mean_x_sentence, _, counts_norm_sentence, _ = histograma(dur_sentencesall, numero_bins=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.01. Modelo nulo duracion distribucion. MEAN FIELD MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FIGURA DEL SUPLEMENTARY INFORMATION 2\n",
    "\n",
    "randLNWord = np.random.lognormal(mean=-1.623, sigma=0.66, size=200000)\n",
    "bin_mean_x_word_rand, _, counts_norm_word_rand, _ = histograma(randLNWord, numero_bins=50, type_bins=\"log\")\n",
    "\n",
    "# WORDS\n",
    "# MODELO LN\n",
    "randSentence4 = [np.random.choice(randLNWord, 2).sum() for x in range(60000)]\n",
    "bin_mean_x_randSentence4, _, counts_norm_randSentence4, _ = histograma(randSentence4, numero_bins=20, type_bins=\"log\")\n",
    "\n",
    "randSentence5 = [np.random.choice(randLNWord, 3).sum() for x in range(60000)]\n",
    "bin_mean_x_randSentence5, _, counts_norm_randSentence5, _ = histograma(randSentence5, numero_bins=20, type_bins=\"log\")\n",
    "\n",
    "randSentence6 = [np.random.choice(randLNWord, 4).sum() for x in range(60000)]\n",
    "bin_mean_x_randSentence6, _, counts_norm_randSentence6, _ = histograma(randSentence6, numero_bins=20, type_bins=\"log\")\n",
    "\n",
    "\n",
    "# MODELO ALEATORIO\n",
    "\n",
    "\n",
    "\n",
    "# Ploteamos\n",
    "f, ax = plt.subplots()\n",
    "# WORDS REAL AND RANDOM\n",
    "ax.plot(bin_mean_x_word, counts_norm_word, 'o', lw = 1, ms = 7, label= 'words', zorder=2, c='darkblue')\n",
    "ax.plot(bin_mean_x_word_rand, counts_norm_word_rand, '--', lw = 2, ms = 1, label= 'LN (-1.623, 0.66)', zorder=1, c=\"k\", alpha=0.8)\n",
    "\n",
    "# BG REAL\n",
    "bin_mean_x_sentence, _, counts_norm_sentence, _ = histograma(dur_sentences, numero_bins=32)\n",
    "ax.plot(bin_mean_x_sentence, counts_norm_sentence, 'd', lw = 2, ms = 7, label= 'BG', zorder=3, c=\"g\")\n",
    "\n",
    "# BG MODEL\n",
    "ax.plot(bin_mean_x_randSentence4, counts_norm_randSentence4, ':', lw = 3, ms = 2, label= '2 Words', zorder=2, c=\"k\", alpha=0.6)\n",
    "ax.plot(bin_mean_x_randSentence5, counts_norm_randSentence5, ':', lw = 3, ms = 2, label= '3 Words', zorder=2, c=\"k\", alpha=0.6)\n",
    "ax.plot(bin_mean_x_randSentence6, counts_norm_randSentence6, ':', lw = 3, ms = 2, label= '4 Words', zorder=2, c=\"k\", alpha=0.6)\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "#ax.set_ylim([0, 11.2])\n",
    "#ax.set_xlim([0, 2])\n",
    "\n",
    "ax.set_xlabel(\"t (seconds)\", fontsize = 12)\n",
    "ax.set_ylabel(\"P(t)\")\n",
    "ax.legend(loc=\"best\", frameon=False)\n",
    "\n",
    "f.savefig(\"SI:2_Mean_Field_model_words_BG.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mismo modelo nuevas 3 figuras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RANDOM LN PHONEME GENERATION\n",
    "randLNphonemes = np.random.lognormal(mean=-2.681, sigma=0.59, size=10000)\n",
    "bin_mean_x_fon_rand, _, counts_norm_fon_rand, _ = histograma(randLNphonemes, numero_bins=60, type_bins=\"log\")\n",
    "\n",
    "\n",
    "# NUMERO DE PHONEMAS POR PALABRA\n",
    "PhonemesPerWord = np.asarray(lista_words.numphonemes)\n",
    "\n",
    "randWord = [np.random.choice(randLNphonemes, np.random.choice(PhonemesPerWord, 1)).sum() for x in range(10000)]#4\n",
    "bin_mean_x_word_rand, _, counts_norm_word_rand, _ = histograma(randWord, numero_bins=35, type_bins=\"log\")\n",
    "\n",
    "\n",
    "# NUMERO DE PALABRAS POR FRASE\n",
    "WordsPerSentence = np.asarray(lista_words.groupby(\"sentence\").size())\n",
    "\n",
    "randSentence = [np.random.choice(randWord, np.random.choice(WordsPerSentence, 1)).sum() for x in range(2000)]\n",
    "bin_mean_x_sentence_rand, _, counts_norm_sentence_rand, _ = histograma(randSentence, numero_bins=30, type_bins=\"log\")\n",
    "\n",
    "bin_mean_x_sentence, _, counts_norm_sentence, _ = histograma(dur_sentences, numero_bins=30)\n",
    "\n",
    "\n",
    "# MODELO PALABRAS POR FRASE INCLUYENDO LA NORMAL\n",
    "randSentenceNORMAL = [np.random.choice(randWord, np.random.choice(WordsPerSentence, 1)).sum() + np.random.normal(0.14, 0.08)  for x in range(30000)]\n",
    "randSentenceNORMAL = np.asarray(randSentenceNORMAL)\n",
    "randSentenceNORMAL = randSentenceNORMAL[randSentenceNORMAL>0.01]\n",
    "bin_mean_x_sentence_randNORMAL, _, counts_norm_sentence_randNORMAL, _ = histograma(randSentenceNORMAL, numero_bins=30, type_bins=\"log\")\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "# # Ploteamos\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "# # PHONEMES real and random\n",
    "ax.plot(bin_mean_x_fon, counts_norm_fon, 's', lw = 1, ms = 9, label= 'phonemes', zorder=1, c=\"darkorange\")\n",
    "xlin = np.linspace(0, max(bin_mean_x_fon), 1000)\n",
    "mu = -2.68\n",
    "sigma = 0.59\n",
    "pdf = (np.exp(-(np.log(xlin) - mu)**2 / (2 * sigma**2)) / (xlin * sigma * np.sqrt(2 * np.pi)))\n",
    "ax.plot(xlin, pdf, '--', lw = 1, ms = 1, label= 'LND fit', zorder=1, c=\"k\", alpha=1)\n",
    "\n",
    "\n",
    "#ax.plot(bin_mean_x_fon_rand, counts_norm_fon_rand, '--', lw = 1, ms = 1, label= 'LN(-2.68, 0.59)', zorder=1, c=\"k\", alpha=1)\n",
    "ax.legend(loc=\"best\", frameon=False, fontsize = 14)\n",
    "ax.set_xlim([0, 0.7])\n",
    "\n",
    "ax.set_xlabel(r\"$t_{ph} (seconds)$\", fontsize = 14)\n",
    "ax.set_ylabel(r\"$P(t_{ph})$\", fontsize = 14)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.set_xticks([0, 0.2, 0.4, 0.6, 0.8])\n",
    "ax.set_yticks([0, 4, 8, 12])\n",
    "\n",
    "f.subplots_adjust(bottom=0.15)\n",
    "\n",
    "f.savefig(\"2_Modelo_P(n)_DuracionPhonemes_Words_BG_1.pdf\")\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylim([0.001, 15])\n",
    "f.tight_layout()\n",
    "f.savefig(\"SI_LOG_Modelo_P(n)_DuracionPhonemes_Words_BG_1.pdf\")\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "# # WORDS\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(bin_mean_x_word, counts_norm_word, 'o', lw = 2, ms = 9, label= 'words', zorder=2, c='darkblue')\n",
    "ax.plot(bin_mean_x_word_rand, counts_norm_word_rand, '--', lw = 2, ms = 1, label= 'theory', zorder=2, c=\"darkblue\", alpha=1)\n",
    "xlin = np.linspace(0, max(bin_mean_x_word), 1000)\n",
    "mu = -1.62331\n",
    "sigma = 0.66019\n",
    "pdf = (np.exp(-(np.log(xlin) - mu)**2 / (2 * sigma**2)) / (xlin * sigma * np.sqrt(2 * np.pi)))\n",
    "ax.plot(xlin, pdf, '--', lw = 1, ms = 1, label= 'LND fit', zorder=2, c=\"k\", alpha=1)\n",
    "\n",
    "\n",
    "\n",
    "# ax.set_ylim([0,4])\n",
    "ax.set_xlim([0, 1.45])\n",
    "\n",
    "ax.set_xlabel(r\"$t_w (seconds)$\", fontsize = 14)\n",
    "ax.set_ylabel(r\"$P(t_w)$\", fontsize = 14)\n",
    "ax.legend(loc=(0.45, 0.14), frameon=False, fontsize = 14)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.set_xticks([0, 0.25, 0.5, 0.75, 1, 1.25])\n",
    "\n",
    "\n",
    "subax = plt.axes([ax.get_position().x1 - 0.3, ax.get_position().y1 - 0.3, .3, .3])\n",
    "PhonemesPerWord = np.asarray(lista_words.numphonemes)\n",
    "from collections import Counter\n",
    "counts = Counter(PhonemesPerWord)\n",
    "counts = sorted(counts.items(), key=lambda pair: pair[0], reverse=False)\n",
    "counts = pd.DataFrame(counts)\n",
    "counts[1]=counts[1]/counts[1].sum()\n",
    "\n",
    "subax.plot(counts[0], counts[1], '--o', lw = 1, ms = 4, zorder=2, c = 'k', alpha = 1)\n",
    "subax.set_xlabel(\"n\", fontsize = 12)\n",
    "subax.set_ylabel(\"P(n)\", fontsize = 12)\n",
    "subax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "subax.set_yscale(\"log\")\n",
    "\n",
    "f.subplots_adjust(bottom=0.15)\n",
    "f.savefig(\"2_Modelo_P(n)_DuracionPhonemes_Words_BG_2.pdf\")\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylim([0.001, 4])\n",
    "ax.legend(loc=(0.1, 0.14), frameon=False, fontsize = 14)\n",
    "f.tight_layout()\n",
    "subax.set_visible(False)\n",
    "\n",
    "f.savefig(\"SI_LOG_Modelo_P(n)_DuracionPhonemes_Words_BG_2.pdf\")\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "ax.plot(bin_mean_x_sentence, counts_norm_sentence, 'd', lw = 2, ms = 9, label= 'BG', zorder=3, c=\"g\")\n",
    "#ax.plot(bin_mean_x_sentence_rand, counts_norm_sentence_rand, '--', lw = 2, ms = 1, label= 'MF model BG P(n)', zorder=3, c=\"k\", alpha=1)\n",
    "ax.plot(bin_mean_x_sentence_randNORMAL, counts_norm_sentence_randNORMAL, '--', lw = 2, ms = 1, label= 'theory', zorder=3, c=\"g\", alpha=1)\n",
    "xlin = np.linspace(0, max(bin_mean_x_sentence), 1000)\n",
    "mu = 0.02523\n",
    "sigma = 0.86027\n",
    "pdf = (np.exp(-(np.log(xlin) - mu)**2 / (2 * sigma**2)) / (xlin * sigma * np.sqrt(2 * np.pi)))\n",
    "ax.plot(xlin, pdf, '--', lw = 1, ms = 1, label= 'LND fit', zorder=2, c=\"k\", alpha=1)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xlim([0, 9])\n",
    "ax.set_ylim([-0.02, 0.8])\n",
    "\n",
    "ax.legend(loc=(0.45, 0.14), frameon=False, fontsize = 14)\n",
    "\n",
    "ax.set_xlabel(r\"$t_{BG} (seconds)$\", fontsize = 14)\n",
    "ax.set_ylabel(r\"$P(t_{BG})$\", fontsize = 14)\n",
    "\n",
    "\n",
    "subax = plt.axes([ax.get_position().x1 - 0.3, ax.get_position().y1 - 0.3, .3, .3])\n",
    "\n",
    "WordsPerSentence = np.asarray(lista_words.groupby(\"sentence\").size())\n",
    "\n",
    "from collections import Counter\n",
    "counts = Counter(WordsPerSentence)\n",
    "counts = sorted(counts.items(), key=lambda pair: pair[0], reverse=False)\n",
    "counts = pd.DataFrame(counts)\n",
    "counts[1]=counts[1]/counts[1].sum()\n",
    "subax.plot(counts[0], counts[1], '--o', lw = 1, ms = 4, zorder=2, c = 'k', alpha = 1)\n",
    "\n",
    "subax.set_xlabel(\"n\", fontsize = 12)\n",
    "subax.set_ylabel(\"W(n)\",fontsize = 12)\n",
    "subax.set_yscale(\"log\")\n",
    "subax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "\n",
    "f.subplots_adjust(bottom=0.15)\n",
    "\n",
    "f.savefig(\"2_Modelo_P(n)_DuracionPhonemes_Words_BG_3.pdf\")\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylim([0.001, 1])\n",
    "ax.legend(loc=(0.1, 0.14), frameon=False, fontsize = 14)\n",
    "\n",
    "f.tight_layout()\n",
    "subax.set_visible(False)\n",
    "\n",
    "f.savefig(\"SI_LOG_Modelo_P(n)_DuracionPhonemes_Words_BG_3.pdf\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Fiteo de las distribuciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiteamos y realizamos los test de si es una lognormal o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words_durations = lista_words.duration.values[(lista_words.duration.values<5) & (lista_words.duration.values>0) ]\n",
    "\n",
    "\n",
    "# FITEO CON LOGNORMAL DE LOS DATOS\n",
    "file = open(\"1_Fit_duration_distribution.txt\",\"w\") \n",
    "\n",
    "# Ajuste fonemas\n",
    "s, loc, scale = scipy.stats.lognorm.fit(fonemes_durations,  floc=0)\n",
    "estimated_mu = np.log(scale)\n",
    "estimated_sigma = s\n",
    "file.write(\"Foneme Duration distribution\\n\")\n",
    "file.write(\"mu: \" + str(np.round(estimated_mu, 5)) + \" sigma:\" + str(np.round(estimated_sigma, 5)))\n",
    "file.write(\"\\n\")\n",
    "file.write(\"\\n\")\n",
    "\n",
    "# Ajuste words\n",
    "s, loc, scale = scipy.stats.lognorm.fit(words_durations,  floc=0)\n",
    "estimated_mu = np.log(scale)\n",
    "estimated_sigma = s\n",
    "file.write(\"Word Duration distribution\\n\")\n",
    "file.write(\"mu: \" + str(np.round(estimated_mu, 5)) + \" sigma:\" + str(np.round(estimated_sigma, 5)))\n",
    "file.write(\"\\n\")\n",
    "file.write(\"\\n\")\n",
    "\n",
    "# Ajuste sentences\n",
    "s, loc, scale = scipy.stats.lognorm.fit(dur_sentences,  floc=0)\n",
    "estimated_mu = np.log(scale)\n",
    "estimated_sigma = s\n",
    "file.write(\"BG Duration distribution\\n\")\n",
    "file.write(\"mu: \" + str(np.round(estimated_mu, 5)) + \" sigma:\" + str(np.round(estimated_sigma, 5)))\n",
    "\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS GOODNESS OF FIT. COMPROBAR SI ES SUFICIENTEMENTE BUENO\n",
    "# Check todas en: http://www.aizac.info/simple-check-of-a-sample-against-80-distributions/\n",
    "file = open(\"1_Goodness_of_fit.txt\",\"w\") \n",
    "cdfs = [\"lognorm\", \"beta\", \"gamma\", \"weibull_min\", \"norm\"]\n",
    "\n",
    "file.write(\"FONEMES\\n\")\n",
    "print(\"PHONEMES\")\n",
    "for cdf in cdfs:\n",
    "    print(cdf)\n",
    "    #fit our data set against every probability distribution\n",
    "    if cdf != \"beta\":\n",
    "        parameters = eval(\"scipy.stats.\"+cdf+\".fit(fonemes_durations, loc=0, scale=1)\")\n",
    "    else:\n",
    "        parameters = eval(\"scipy.stats.\"+cdf+\".fit(fonemes_durations)\")\n",
    "    #Applying the Kolmogorov-Smirnof one sided test\n",
    "    D, p = scipy.stats.kstest(fonemes_durations, cdf, args=parameters)\n",
    "    D = np.round(D, 3)\n",
    "    #pretty-print the results\n",
    "    file.write(str(cdf.ljust(16)) + (\"p: \"+str(p)).ljust(25)+\"D: \"+str(D) + \"\\n\")\n",
    "file.write(\"\\n\")\n",
    "file.write(\"\\n\")\n",
    "\n",
    "print(\"WORDS\")\n",
    "\n",
    "file.write(\"WORDS\\n\")\n",
    "for cdf in cdfs:\n",
    "    print(cdf)\n",
    "    #fit our data set against every probability distribution\n",
    "    if cdf != \"beta\":\n",
    "        parameters = eval(\"scipy.stats.\"+cdf+\".fit(words_durations, loc=0, scale=1)\")\n",
    "    else:\n",
    "        parameters = eval(\"scipy.stats.\"+cdf+\".fit(words_durations)\")\n",
    "    #Applying the Kolmogorov-Smirnof one sided test\n",
    "    D, p = scipy.stats.kstest(words_durations, cdf, args=parameters)\n",
    "    D = np.round(D, 3)\n",
    "    #pretty-print the results\n",
    "    file.write(str(cdf.ljust(16)) + (\"p: \"+str(p)).ljust(25)+\"D: \"+str(D) + \"\\n\")\n",
    "file.write(\"\\n\")\n",
    "file.write(\"\\n\")\n",
    "\n",
    "print(\"BG\")\n",
    "\n",
    "file.write(\"BG\\n\")\n",
    "for cdf in cdfs:\n",
    "    print(cdf)\n",
    "    #fit our data set against every probability distribution\n",
    "    if cdf != \"beta\":\n",
    "        parameters = eval(\"scipy.stats.\"+cdf+\".fit(dur_sentences, loc=0, scale=1)\")\n",
    "    else:\n",
    "        parameters = eval(\"scipy.stats.\"+cdf+\".fit(dur_sentences)\")\n",
    "    #Applying the Kolmogorov-Smirnof one sided test\n",
    "    D, p = scipy.stats.kstest(dur_sentences, cdf, args=parameters)\n",
    "    D = np.round(D, 3)\n",
    "    #pretty-print the results\n",
    "    file.write(str(cdf.ljust(16)) + (\"p: \"+str(p)).ljust(25)+\"  D: \"+str(D) + \"\\n\")\n",
    "file.write(\"\\n\")\n",
    "file.write(\"\\n\")\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max likelihood\n",
    "file = open(\"1_Likelihood.txt\",\"w\") \n",
    "cdfs = [\"lognorm\", \"beta\", \"gamma\", \"weibull_min\", \"norm\"]\n",
    "\n",
    "file.write(\"PHONEMES\\n\")\n",
    "refvalue = 0\n",
    "print(\"PHONEMES\")\n",
    "for cdf in cdfs:\n",
    "    print(cdf)\n",
    "    if cdf == \"gamma\":\n",
    "        dist = getattr(scipy.stats, cdf)\n",
    "        params = dist.fit(fonemes_durations, loc = 0)\n",
    "\n",
    "    else:\n",
    "        dist = getattr(scipy.stats, cdf)\n",
    "        params = dist.fit(fonemes_durations)\n",
    "    LLH = dist.logpdf(fonemes_durations,*params).sum() / len(fonemes_durations)\n",
    "    LLH = np.round(LLH, 2)\n",
    "    # pretty-print the results\n",
    "    file.write(str(cdf.ljust(16)) + \"LLH: \"+str(LLH) + \"\\n\")\n",
    "file.write(\"\\n\")\n",
    "file.write(\"\\n\")\n",
    "\n",
    "file.write(\"WORDS\\n\")\n",
    "print(\"WORDS\")\n",
    "\n",
    "for cdf in cdfs:\n",
    "    print(cdf)\n",
    "    dist = getattr(scipy.stats, cdf)\n",
    "    params = dist.fit(words_durations)\n",
    "    LLH = dist.logpdf(words_durations,*params).sum() / len(words_durations)\n",
    "    LLH = np.round(LLH, 2)\n",
    "    # pretty-print the results\n",
    "    file.write(str(cdf.ljust(16)) + \"LLH: \"+str(LLH) + \"\\n\")\n",
    "file.write(\"\\n\")\n",
    "file.write(\"\\n\")\n",
    "\n",
    "\n",
    "file.write(\"BG\\n\")\n",
    "print(\"bg\")\n",
    "\n",
    "for cdf in cdfs:\n",
    "    print(cdf)\n",
    "    dist = getattr(scipy.stats, cdf)\n",
    "    params = dist.fit(dur_sentences)\n",
    "    LLH = dist.logpdf(dur_sentences,*params).sum() / len(dur_sentences)\n",
    "    LLH = np.round(LLH, 2)\n",
    "    # pretty-print the results\n",
    "    file.write(str(cdf.ljust(16)) + \"LLH: \"+str(LLH) + \"\\n\")\n",
    "file.write(\"\\n\")\n",
    "file.write(\"\\n\")\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Informacion mutua de los fonemas\n",
    "En primer lugar hay que simbolizar las duraciones de las palabras. En vez de tener una duracion infinita, hacemos una simbolizacion de un numero finito de elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Simbolizamos los niveles\n",
    "def myround(x, base=0.03):\n",
    "    return np.round(float(base * round(float(x)/base)), 3)\n",
    "\n",
    "tablFonMutualInformation = tabla_datos[tabla_datos.tipe == \"p\"]\n",
    "tablFonMutualInformation.loc[:, \"duration\"] = tablFonMutualInformation.duration.apply(myround)\n",
    "tablFonMutualInformation.duration[tablFonMutualInformation.duration>0.24] = 0.24\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define una funcion que dado un array de n listas hace lo siguiente:\n",
    "- Convierte las n listas de n elementos en listas de dos elementos.\n",
    "- Calcula las probabilidades de cada par (x1, x2)\n",
    "- Calcula la probabilidad de cada (x)\n",
    "- Calcula la informacion mutua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import more_itertools as mit\n",
    "def InformacionMutua(lista_tuples):\n",
    "    # La informacion mutua se calcula en base 2\n",
    "    # Convertir las listas de n elementos en otra lista de n tuples.\n",
    "    lista = []\n",
    "    listaelemtosindividual = []\n",
    "    for item in lista_tuples:\n",
    "        if len(item)>1:\n",
    "            lista = lista + list(mit.windowed(item, n=2, step=1))\n",
    "            listaelemtosindividual = listaelemtosindividual + item\n",
    "\n",
    "    # Calculamos los diccionarios de probabilidades independientes y conjuntas\n",
    "    dfJoinProbs = pd.DataFrame.from_dict(Counter(lista), orient='index').reset_index()\n",
    "    dfJoinProbs.rename(columns={'index':'element', 0:'prob'}, inplace=True)\n",
    "    dfJoinProbs.loc[:, \"prob\"] = dfJoinProbs.prob/dfJoinProbs.prob.sum() # Normalize\n",
    "\n",
    "    dfIndepProbs = pd.DataFrame.from_dict(Counter(listaelemtosindividual), orient='index').reset_index()\n",
    "    dfIndepProbs.rename(columns={'index':'element', 0:'prob'}, inplace=True)\n",
    "    dfIndepProbs.loc[:,\"prob\"] = dfIndepProbs.prob/dfIndepProbs.prob.sum() # Normalize\n",
    "\n",
    "    # Calculo de la informacion mutua media\n",
    "    l_mutual_information = []\n",
    "    for _, item in dfJoinProbs.iterrows():\n",
    "        probt1t2 = item.prob\n",
    "        probt1 = float(dfIndepProbs[dfIndepProbs.element == item.element[0]].prob)\n",
    "        probt2 = float(dfIndepProbs[dfIndepProbs.element == item.element[1]].prob)\n",
    "        mi = probt1t2 * np.log2(probt1t2/(probt1*probt2))\n",
    "        l_mutual_information.append(mi)\n",
    "\n",
    "    informacion_mutua = np.sum(l_mutual_information)\n",
    "    return(informacion_mutua)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namefichero = \"2_Informacion_mutua_fonemas.txt\"\n",
    "file = open(namefichero, \"w\") \n",
    "\n",
    "# Agrupar por token y convertir cada palabra a una lista de duraciones\n",
    "DurationLista = np.array(tablFonMutualInformation.groupby(\"numtoken\")[\"duration\"].apply(list))\n",
    "print(\"calculando informacion mutua\")\n",
    "imutua = InformacionMutua(DurationLista)\n",
    "print(\"informacion mutua: \" + str(imutua))\n",
    "\n",
    "file.write(\"Informacion mutua real: \" + str(imutua) + \"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero realizamos una copia del elemento\n",
    "randomtablFonMutualInformation  = tablFonMutualInformation.copy()\n",
    "\n",
    "# This could take a while...\n",
    "\n",
    "for i in np.arange(0, 10):\n",
    "    print(i)\n",
    "    # Se extrae la columna que indica a que palabra pertenece y se randomiza\n",
    "    column = tablFonMutualInformation.numtoken.sample(frac=1.0) # shuffle\n",
    "    column.reset_index(inplace=True, drop=True)\n",
    "    randomtablFonMutualInformation.drop(columns= \"numtoken\", inplace=True)\n",
    "    randomtablFonMutualInformation[\"numtoken\"] = np.array(column)\n",
    "    \n",
    "    RANDDurationLista = np.array(randomtablFonMutualInformation.groupby(\"numtoken\")[\"duration\"].apply(list))\n",
    "    print(\"calculando informacion mutua\")\n",
    "    RANDimutua = InformacionMutua(RANDDurationLista)\n",
    "    print(\"informacion mutua: \" + str(RANDimutua))\n",
    "    \n",
    "    file = open(namefichero,\"a\")\n",
    "    file.write(\"Informacion mutua random: \" + str(RANDimutua) + \"\\n\")\n",
    "    file.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Informacion mutua palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simbolizamos los niveles\n",
    "def myround(x, base=0.10):\n",
    "    return np.round(float(base * round(float(x)/base)), 2)\n",
    "\n",
    "tablWordMutualInformation = tabla_datos[tabla_datos.tipe == \"w\"]\n",
    "tablWordMutualInformation.loc[:, \"duration\"] = tablWordMutualInformation.duration.apply(myround)\n",
    "tablWordMutualInformation.duration[tablWordMutualInformation.duration>0.7] = 0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namefichero = \"2_Informacion_mutua_words.txt\"\n",
    "file = open(namefichero, \"w\") \n",
    "\n",
    "# Agrupar por token y convertir cada palabra a una lista de duraciones\n",
    "DurationLista = np.array(tablWordMutualInformation.groupby(\"sentence\")[\"duration\"].apply(list))\n",
    "print(\"calculando informacion mutua\")\n",
    "imutua = InformacionMutua(DurationLista)\n",
    "print(\"informacion mutua: \" + str(imutua))\n",
    "file.write(\"Informacion mutua real: \" + str(imutua) + \"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero realizamos una copia del elemento\n",
    "randomtablWordMutualInformation  = tablWordMutualInformation.copy()\n",
    "\n",
    "for i in np.arange(0, 10):\n",
    "    print(i)\n",
    "    # Se extrae la columna que indica a que palabra pertenece y se randomiza\n",
    "    column = tablWordMutualInformation.sentence.sample(frac=1.0) # shuffle\n",
    "    column.reset_index(inplace=True, drop=True)\n",
    "    randomtablWordMutualInformation.drop(columns= \"sentence\", inplace=True)\n",
    "    randomtablWordMutualInformation[\"sentence\"] = np.array(column)\n",
    "    \n",
    "    RANDDurationLista = np.array(randomtablWordMutualInformation.groupby(\"sentence\")[\"duration\"].apply(list))\n",
    "    print(\"calculando informacion mutua\")\n",
    "    RANDimutua = InformacionMutua(RANDDurationLista)\n",
    "    print(\"informacion mutua: \" + str(RANDimutua))\n",
    "    \n",
    "    file = open(namefichero,\"a\")\n",
    "    file.write(\"Informacion mutua random: \" + str(RANDimutua) + \"\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6. Distribucion numero de fonemas por palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PhonemesPerWord = np.asarray(lista_words.numphonemes)\n",
    "from collections import Counter\n",
    "counts = Counter(PhonemesPerWord)\n",
    "counts = sorted(counts.items(), key=lambda pair: pair[0], reverse=False)\n",
    "counts = pd.DataFrame(counts)\n",
    "counts[1]=counts[1]/counts[1].sum()\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(counts[0], counts[1], '--o', lw = 1, ms = 7, zorder=2, c = 'k', alpha = 1)\n",
    "#ax.set_xscale(\"log\")\n",
    "#ax.legend()\n",
    "# print(counts)\n",
    "ax.set_xlabel(\"number of phonemes (n)\")\n",
    "ax.set_ylabel(\"P(n)\")\n",
    "f.savefig(\"SI:3_Number_of_phonemes_per_word.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. Distribucion numero de palabras por breathe group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordsPerSentence = np.asarray(lista_words.groupby(\"sentence\").size())\n",
    "\n",
    "from collections import Counter\n",
    "counts = Counter(WordsPerSentence)\n",
    "counts = sorted(counts.items(), key=lambda pair: pair[0], reverse=False)\n",
    "counts = pd.DataFrame(counts)\n",
    "counts[1]=counts[1]/counts[1].sum()\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(counts[0], counts[1], '--o', lw = 1, ms = 7, zorder=2, c = 'k', alpha = 1)\n",
    "#ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "ax.set_xlabel(\"number of words (n)\")\n",
    "ax.set_ylabel(\"P(n)\")\n",
    "f.savefig(\"SI:4_Number_of_words_per_BG.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8. Distribucion numero de caracteres por palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numcharperword = np.array(lista_words.numletters)\n",
    "\n",
    "from collections import Counter\n",
    "counts = Counter(numcharperword)\n",
    "counts = sorted(counts.items(), key=lambda pair: pair[0], reverse=False)\n",
    "counts = pd.DataFrame(counts)\n",
    "counts[1]=counts[1]/counts[1].sum()\n",
    "\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(counts[0], counts[1], '--o', lw = 1, ms = 7, zorder=2, c = 'k', alpha = 1)\n",
    "#ax.set_yscale(\"log\")\n",
    "\n",
    "ax.set_xlim([0.5, 18.5])\n",
    "ax.set_xticks([1,5,9,13,17])\n",
    "\n",
    "ax.set_xlabel(\"number of characters (n)\")\n",
    "ax.set_ylabel(\"P(n)\")\n",
    "f.savefig(\"SI:5_Number_of_characters_per_word.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9. Numero de caracteres por fonema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numcharperphoneme = lista_words.numletters/lista_words.numphonemes\n",
    "numcharperphoneme = numcharperphoneme[numcharperphoneme<7]\n",
    "bin1, _, counts, _ = histograma(numcharperphoneme, 6, \"lin\")\n",
    "\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(np.round(bin1), counts, '--o', lw = 1, ms = 7, zorder=2, c = 'k', alpha = 1)\n",
    "#ax.set_yscale(\"log\")\n",
    "\n",
    "ax.set_xlim([0.5, 5.5])\n",
    "ax.set_xticks([1,2,3,4,5])\n",
    "\n",
    "ax.set_xlabel(\"number of characters (n)\")\n",
    "ax.set_ylabel(\"P(n)\")\n",
    "f.savefig(\"SI:6_Number_of_characters_per_phoneme.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ley de Zipf\n",
    "Calculamos la ley de zip (corpus escrito) para el articulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_fonema_token_process = lista_fonema.token.str.split(\";\", expand=True)[0]\n",
    "lista_fonema_token_process = lista_fonema_token_process.str.split(\"+\", expand=True)[0]\n",
    "\n",
    "\n",
    "freqs_words = lista_words.token.value_counts()\n",
    "freqs_phonemes = lista_fonema_token_process.value_counts()\n",
    "\n",
    "freqs_words.index = np.arange(1, len(freqs_words) + 1)\n",
    "\n",
    "freqs_phonemes.index = np.arange(1, len(freqs_phonemes) + 1)\n",
    "\n",
    "\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(freqs_words[0:30000], 'o', lw = 2, ms = 8, label= 'words', zorder=2, alpha=1, c='darkblue')\n",
    "ax.plot(freqs_phonemes[0:43], 's', lw = 2, ms = 8, label= 'phonemes', zorder=1,alpha=1, c='darkorange')\n",
    "\n",
    "\n",
    "# PENDIENTE PHONEMAS\n",
    "xphoneme = freqs_phonemes.index.values\n",
    "yplotphon = xphoneme**(-0.25190558) * 0.96130884**xphoneme\n",
    "yplotphon = yplotphon/yplotphon[0]*freqs_phonemes.values[0]\n",
    "ax.plot(xphoneme, yplotphon, \"--\", color=\"k\", lw = 2)\n",
    "ax.text(0.29, 0.92, \"$Yule(0.25, 0.96)$\", horizontalalignment='center', fontsize = 11, verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# WORDS\n",
    "\n",
    "# Line pendiente\n",
    "xline1 = np.array([1, 50])\n",
    "yline1 = np.power(6.5*10**-7*xline1, -0.6243)\n",
    "yline1 = yline1 * 2\n",
    "\n",
    "ax.plot(xline1, yline1 , \"--\", color = \"lightgray\", lw = 3)\n",
    "ax.text(0.2, 0.53, r'$\\alpha_1 = 0.63$', horizontalalignment='center', fontsize = 12, verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "xline2 = np.array([49, 8000])\n",
    "yline2 = np.power(7*10**-5*xline2, -1.4107604)\n",
    "yline2 = yline2/yline2[0]*yline1[-1]\n",
    "\n",
    "ax.plot(xline2, yline2, \"--\", color = \"lightgray\", lw = 3)\n",
    "ax.text(0.57, 0.22, r'$\\alpha_2 = 1.41$', horizontalalignment='center', fontsize = 12, verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "ax.annotate(r'$r=49$', xy=(49, yline2[0]), xytext=(20, yline2[0]*0.09), \n",
    "            arrowprops=dict(facecolor='black', shrink=0.01, width=0.01, headwidth=4),\n",
    "           fontsize=12)\n",
    "\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "ax.set_xlabel(r\"$rank$\",fontsize=14)\n",
    "ax.set_ylabel(r\"$Freq(r)$\", fontsize=14)\n",
    "ax.legend(fontsize=13, frameon=False, loc=3)\n",
    "ax.set_ylim([0.3, 4*10**5])\n",
    "\n",
    "f.savefig(\"2_Zipf_law.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check individual speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_words[\"informant\"] = lista_words.path.str[0:3]\n",
    "informants = [df for _, df in lista_words.groupby(\"informant\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_words\n",
    "for i in range(10):\n",
    "    freqs_words = informants[i].token.value_counts()\n",
    "\n",
    "    freqs_words.index = np.arange(1, len(freqs_words) + 1)\n",
    "\n",
    "\n",
    "\n",
    "    f, ax = plt.subplots()\n",
    "    ax.plot(freqs_words[0:30000], 'o', lw = 2, ms = 8, label= 'words', zorder=2, alpha=1, c='darkblue')\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    \n",
    "    ax.set_xlabel(r\"$rank$\",fontsize=14)\n",
    "    ax.set_ylabel(r\"$Freq(r)$\", fontsize=14)\n",
    "    \n",
    "    f.savefig(\"SI_Zipf_informant_\" +str(i)+ \"_law.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Modelo Nulo Ley de Zipf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploteamos\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "\n",
    "words_durations = lista_words.duration.values[(lista_words.duration.values<5) & (lista_words.duration.values>0) ]\n",
    "\n",
    "# Realizamos los histogramas\n",
    "bin_mean_x, counts_mean, counts_norm, bin_edges = histograma(words_durations, numero_bins=3000, type_bins=\"lin\")\n",
    "\n",
    "freqs = np.sort(counts_mean)[::-1]\n",
    "ax.plot(freqs, 'o', lw = 2, ms = 8, zorder=2, alpha=0.3, color='k')\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "\n",
    "ax.yaxis.set_label_position(\"left\")\n",
    "ax.yaxis.set_ticks_position(\"left\")\n",
    "\n",
    "#subax.set_xlim([0.1,8.5])\n",
    "ax.xaxis.set_label_position(\"bottom\")\n",
    "ax.xaxis.set_ticks_position(\"bottom\")\n",
    "\n",
    "ax.set_xlabel(r\"$rank$\",fontsize=14)\n",
    "ax.set_ylabel(r\"$Freq(r)$\", fontsize=14,  rotation=90)\n",
    "\n",
    "# ax.set_xticks([10**0,10**2])\n",
    "# ax.set_yticks([10**4,10**6])\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "f.savefig(\"SI_Zipf_Nul_Model.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipf Doble Power Law ajuste MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "# DATA\n",
    "x = freqs_words.index.values[0::]\n",
    "y = freqs_words.values[0::]\n",
    "y = y/y.sum() # Normalizo a probabilidades\n",
    "\n",
    "f,ax = plt.subplots()\n",
    "ax.plot(x, y, 'o')\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "#####################\n",
    "def loglik(parameters):\n",
    "    alpha = parameters[0]\n",
    "    beta = parameters[1]\n",
    "\n",
    "    Probabilities1 = list(x[0:cut]**(-alpha))\n",
    "    \n",
    "    # Conecto con el anterior\n",
    "    Probabilities2 = list(x[cut-1::]**(-beta))\n",
    "    Probabilities2 = Probabilities2/Probabilities2[0]*Probabilities1[-1]\n",
    "\n",
    "    # Elimino el punto de conexion\n",
    "    Probabilities2 = list(Probabilities2[1::])    \n",
    "    \n",
    "    Probabilities = Probabilities1 + Probabilities2\n",
    "    Probabilities = np.asarray(Probabilities)\n",
    "    # Normalized\n",
    "    Probabilities = Probabilities/Probabilities.sum()\n",
    "    \n",
    "    # Log Likelihoood\n",
    "    Lvector = np.log(Probabilities)\n",
    "    \n",
    "    # Multiply the vector by frequencies\n",
    "    Lvector = np.log(Probabilities) *  freqs_words.values\n",
    "    \n",
    "    # LL is the sum\n",
    "    L = Lvector.sum()\n",
    "    \n",
    "    # We want to maximize LogLikelihood or minimize (-1)*LogLikelihood\n",
    "    return(-L)\n",
    "\n",
    "\n",
    "# INICIALIZACION\n",
    "bestcut = 0\n",
    "ll_best = 10**10\n",
    "\n",
    "for cut in range(30,100):\n",
    "    mle = minimize(loglik, np.asarray([2,3]))\n",
    "    if mle.fun < ll_best:\n",
    "        mle_best = mle\n",
    "        ll_best = mle.fun\n",
    "        bestcut = cut\n",
    "     \n",
    "\n",
    "print(\"  \")\n",
    "print(\"Mejor Resultados\")    \n",
    "print(bestcut)\n",
    "print(mle_best)\n",
    "\n",
    "\n",
    "x1 = x[0:bestcut]\n",
    "x2 = x[bestcut-1::]\n",
    "\n",
    "y1 = 0.05*x1 **(-mle_best['x'][0])\n",
    "y2 = x2 **(-mle_best['x'][1])\n",
    "\n",
    "ax.plot(x1, y1)\n",
    "ax.plot(x2, y2/y2[0]*y1[-1])\n",
    "\n",
    "\n",
    "################################################################\n",
    "# CHI SQUARE ###################################################\n",
    "\n",
    "from scipy.stats import chisquare\n",
    "from scipy import stats\n",
    "\n",
    "Probabilities1 = list(x[0:cut]**(-mle_best['x'][0]))\n",
    "# Conecto con el anterior\n",
    "Probabilities2 = list(x[cut-1::]**(-mle_best['x'][1]))\n",
    "Probabilities2 = Probabilities2/Probabilities2[0]*Probabilities1[-1]\n",
    "# Elimino el punto de conexion\n",
    "Probabilities2 = list(Probabilities2[1::])    \n",
    "Probabilities = Probabilities1 + Probabilities2\n",
    "Probabilities = np.asarray(Probabilities)\n",
    "# Normalized\n",
    "ProbabilitiesExpected = Probabilities/Probabilities.sum()\n",
    "\n",
    "\n",
    "FreqsReal = freqs_words.values\n",
    "ProbabilitiesExpected = ProbabilitiesExpected*FreqsReal.sum()\n",
    "ProbabilitiesExpected = ProbabilitiesExpected.round()\n",
    "\n",
    "FreqsReal = FreqsReal + 1\n",
    "ProbabilitiesExpected = ProbabilitiesExpected + 1\n",
    "\n",
    "testchi = chisquare(ProbabilitiesExpected/3, f_exp=FreqsReal/3, ddof = 3)\n",
    "\n",
    "# The chi square test tests the null hypothesis that the categorical data has the given frequencies.\n",
    "\n",
    "\n",
    "BIC2 = 2*np.log(freqs_words.values.sum()) - 2*(-mle_best.fun)\n",
    "print(BIC2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste fonemas Zipf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax.plot(freqs_phonemes[0:43], 's', lw = 2, ms = 8, label= 'phonemes', zorder=1,alpha=1, c='darkorange')\n",
    "\n",
    "\n",
    "# DATA\n",
    "x = freqs_phonemes.index.values[0:43]\n",
    "y = freqs_phonemes.values[0:43]\n",
    "y = y/y.sum()\n",
    "\n",
    "\n",
    "x = x[0::]\n",
    "y = y[0::]\n",
    "y = y/y.sum()\n",
    "\n",
    "f,ax = plt.subplots()\n",
    "ax.plot(x, y, 'o')\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "#####################\n",
    "def loglik(param): \n",
    "    # Calcula el MeanLogLikelihoood de los datos\n",
    "    b = param[0]\n",
    "    c =param[1]\n",
    "    # Power law function\n",
    "    Probabilities = x**(-b)*c**x\n",
    "    \n",
    "    # Normalized\n",
    "    Probabilities = Probabilities/Probabilities.sum()\n",
    "    \n",
    "    # Log Likelihoood\n",
    "    #Lvector = np.log(Probabilities)\n",
    "    \n",
    "    # Multiply the vector by probabilities\n",
    "    Lvector = np.log(Probabilities) * y\n",
    "    \n",
    "    # LL is the sum\n",
    "    L = Lvector.sum()\n",
    "    \n",
    "    # Normalizo por rangos\n",
    "    #L = L/len(x)\n",
    "    \n",
    "    # We want to maximize LogLikelihood or minimize (-1)*LogLikelihood\n",
    "    return(-L)\n",
    "\n",
    "s_best = minimize(loglik, np.asarray([2,1]))\n",
    "print(s_best)\n",
    "ax.plot(x, y[0]*x**(-s_best.x[0])* (s_best.x[1]**x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "# CHI SQUARE ###################################################\n",
    "\n",
    "from scipy.stats import chisquare\n",
    "from scipy import stats\n",
    "\n",
    "Probabilities = x**(-s_best.x[0])* (s_best.x[1]**x)\n",
    "ProbabilitiesExpected = Probabilities/Probabilities.sum()\n",
    "\n",
    "FreqsReal = freqs_phonemes.values[0:43]\n",
    "\n",
    "ProbabilitiesExpected = ProbabilitiesExpected*FreqsReal.sum()\n",
    "ProbabilitiesExpected = ProbabilitiesExpected.round()\n",
    "\n",
    "FreqsReal = FreqsReal + 1\n",
    "ProbabilitiesExpected = ProbabilitiesExpected + 1\n",
    "\n",
    "testchi = chisquare(ProbabilitiesExpected/1000, f_exp=FreqsReal/1000, ddof = 2)\n",
    "\n",
    "# The chi square test tests the null hypothesis that the categorical data has the given frequencies.\n",
    "testchi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Brevity law \n",
    "Calculamos la ley de brevedad chequeando que las palabras mÃ¡s recuentes tienden a ser mÃ¡s cortas\n",
    "Entendemos tres posibles definiciones:\n",
    "\n",
    "\n",
    "## 4.1 WORDS\n",
    "1. Frequencia palabras vs duracion en segundos\n",
    "2. Frecuencia palabras vs numero de fonemas\n",
    "3. Frecuencia de palabras vs numero de caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creamos una columna que indique las veces que ha ocurrido esa palabra en nuestra base de datos\n",
    "lista_words = lista_words.assign(repetitions=lista_words.token.map(lista_words.token.value_counts()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def func_exp(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "def func_log(x, a, b): # FUNCION LOGARITMICA que utiliza los mismos parametros\n",
    "    return 1/(-b) * ( np.log(x) - np.log(a) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1. Frequencia palabras vs duracion segundos\n",
    "#brevity0 = lista_words.groupby(\"repetitions\").duration.mean()\n",
    "brevity0 = lista_words.groupby(\"token\").median()[[\"duration\", \"repetitions\"]]\n",
    "\n",
    "\n",
    "#The two-sided p-value for a hypothesis test whose null hypothesis is that two sets of data are uncorrelated, \n",
    "#A small p-value (typically â¤ 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis\n",
    "print(\"Frequencia palabras vs duracion(segundos:\")\n",
    "print(scipy.stats.spearmanr(brevity0.duration, np.log(brevity0.repetitions)))\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "\n",
    "### AHORA VAMOS A HACER UN BINING\n",
    "freq0, pos0  = binear_datos(brevity0.repetitions, brevity0.duration, bins=28, log = True)\n",
    "\n",
    "ax.plot(brevity0.duration, brevity0.repetitions, 'o', lw = 1, ms = 1, zorder=2, alpha=0.6, color = 'lightgray',fillstyle= \"none\")\n",
    "ax.plot(pos0, freq0, 'o', lw = 2, ms = 10, label= 'words', zorder=2, alpha=1, color=\"darkblue\")\n",
    "\n",
    "ax.set_xlabel(r\"$Median$\"+ \" \" + r\"$duration$\"+ \" \" + r\"$(seconds)$\", fontsize=13)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=13)\n",
    "ax.set_yscale(\"log\")\n",
    "#ax.set_xscale(\"log\")\n",
    "ax.set_xlim([0,0.7])\n",
    "ax.set_ylim([0.5,5*10**4])\n",
    "\n",
    "\n",
    "\n",
    "## FITING\n",
    "x = brevity0.repetitions\n",
    "y = brevity0.duration\n",
    "\n",
    "xplot = np.linspace(0.05, 0.55, 8)\n",
    "popt, pcov = curve_fit(func_log, x, y)\n",
    "ax.plot(xplot, func_exp(xplot, *popt), \"--\", color = \"darkred\", lw = 2)\n",
    "\n",
    "ax.text(0.72, 0.38, r'$\\lambda = $' + \"{:.{}f}\".format(popt[1], 1), horizontalalignment='center', fontsize = 15, verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "residuals = y - func_log(x, *popt)\n",
    "ss_res = np.sum(residuals**2)\n",
    "ss_tot = np.sum((y-np.mean(y))**2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "print(\"R2: \" + str(r_squared))\n",
    "print()\n",
    "\n",
    "\n",
    "##############################################\n",
    "## 2.frecuencia palabras vs numero de fonemas\n",
    "##############################################\n",
    "def func_exp64(x, a, b):\n",
    "    return a * 64**(-b * x)\n",
    "\n",
    "def func_log64(x, a, b): # FUNCION LOGARITMICA que utiliza los mismos parametros\n",
    "    return 1/(-b) * ( np.log(x)/np.log(64) - np.log(a)/np.log(64) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "brevity1 = lista_words.groupby(\"token\").median()[[\"numphonemes\", \"repetitions\"]]\n",
    "\n",
    "print(\"Frequencia palabras vs numero de fonemas:\")\n",
    "print(scipy.stats.spearmanr(brevity1.numphonemes, np.log(brevity1.repetitions)))\n",
    "\n",
    "\n",
    "# Ploteamos\n",
    "subax = plt.axes([ax.get_position().x1 - .28, ax.get_position().y1 - .28, .28, .28])\n",
    "\n",
    "freq1, pos1 = binear_datos(brevity1.repetitions, brevity1.numphonemes, bins=18, log = True)\n",
    "subax.plot(brevity1.numphonemes, brevity1.repetitions, 'o', lw = 1, ms = 2, zorder=2, alpha=0.6, color = 'lightgray', fillstyle= \"none\")\n",
    "subax.plot(pos1, freq1, 'o', lw = 2, ms = 7, label= 'words', zorder=2, alpha=1, color=\"darkblue\")\n",
    "\n",
    "\n",
    "\n",
    "## FITING#######################################################\n",
    "x = brevity1.repetitions\n",
    "y = brevity1.numphonemes\n",
    "\n",
    "xplot = np.linspace(1.5, 8, 3)\n",
    "popt, pcov = curve_fit(func_log64, x, y)\n",
    "subax.plot(xplot, func_exp64(xplot, *popt), \"--\", color = \"darkred\", lw = 2)\n",
    "subax.text(0.74, 0.73, r'$\\lambda_D = $' + \"{:.{}f}\".format(popt[1], 1), horizontalalignment='center', fontsize = 12, verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "residuals = y - func_log64(x, *popt)\n",
    "ss_res = np.sum(residuals**2)\n",
    "ss_tot = np.sum((y-np.mean(y))**2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "print(\"R2: \" + str(r_squared))\n",
    "print()\n",
    "\n",
    "\n",
    "subax.set_yscale(\"log\")\n",
    "subax.set_xlabel(r\"$\\widetilde{size}$\" + \" \" +  r\"$(phonemes)$\", fontsize=12)\n",
    "subax.set_xlim([0.5, 7])\n",
    "subax.set_ylim([0.3, 50000])\n",
    "#subax.set_ylabel(\"freq\",rotation=0)\n",
    "\n",
    "\n",
    "##############################################\n",
    "## 3.frecuencia palabras vs numero de characters\n",
    "##############################################\n",
    "def func_exp26(x, a, b):\n",
    "    return a * 26**(-b * x)\n",
    "\n",
    "def func_log26(x, a, b): # FUNCION LOGARITMICA que utiliza los mismos parametros\n",
    "    return 1/(-b) * ( np.log(x)/np.log(26) - np.log(a)/np.log(26) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "brevity2 = lista_words.groupby(\"token\").median()[[\"numletters\", \"repetitions\"]]\n",
    "\n",
    "brevity2=brevity2[brevity2.numletters<15]\n",
    "print(\"Frequencia palabras vs numero de caracteres:\")\n",
    "print(scipy.stats.spearmanr(brevity2.numletters, np.log(brevity2.repetitions)))\n",
    "\n",
    "\n",
    "# Ploteamos\n",
    "subax2 = plt.axes([ax.get_position().x0, ax.get_position().y0, .28, .28])\n",
    "\n",
    "freq2, pos2 = binear_datos(brevity2.repetitions, brevity2.numletters, bins=13, log = True)\n",
    "subax2.plot(brevity2.numletters, brevity2.repetitions, 'o', lw = 1, ms = 2, zorder=2, alpha=0.4, color = 'lightgrey')\n",
    "subax2.plot(pos2, freq2, 'o', lw = 2, ms = 7, label= 'words', zorder=2, alpha=1, color=\"darkblue\")\n",
    "\n",
    "\n",
    "## FITING#######################################################\n",
    "x = brevity2.repetitions\n",
    "y = brevity2.numletters\n",
    "\n",
    "xplot = np.linspace(2, 12, 3)\n",
    "popt, pcov = curve_fit(func_log26, x, y)\n",
    "subax2.plot(xplot, func_exp26(xplot, *popt), \"--\", color = \"darkred\", lw = 2)\n",
    "subax2.text(0.12, 0.12, r'$\\lambda_D = $' + \"{:.{}f}\".format(popt[1], 1), horizontalalignment='center', fontsize = 12, verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "\n",
    "residuals = y - func_log26(x, *popt)\n",
    "ss_res = np.sum(residuals**2)\n",
    "ss_tot = np.sum((y-np.mean(y))**2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "print(\"R2: \" + str(r_squared))\n",
    "\n",
    "\n",
    "\n",
    "subax2.set_yscale(\"log\")\n",
    "#subax2.set_xscale(\"log\")\n",
    "\n",
    "subax2.set_xlabel(r\"$size$\" + \" \" +  r\"$(chars)$\",rotation=0, fontsize=12)\n",
    "subax2.set_xlim([0.5, 8])\n",
    "subax2.set_ylim([0.2, 10**5])\n",
    "\n",
    "subax2.yaxis.set_label_position(\"right\")\n",
    "subax2.yaxis.set_ticks_position(\"right\")\n",
    "\n",
    "#subax2.set_ylabel(\"freq\",rotation=0)\n",
    "subax2.xaxis.set_label_position(\"top\")\n",
    "subax2.xaxis.set_ticks_position(\"top\")\n",
    "\n",
    "\n",
    "\n",
    "subax2.set_xticks([1,3,5,7])\n",
    "subax2.set_yticks([10**0,10**2, 10**4])\n",
    "\n",
    "#subax2.set_yticks([10**3,10**5])\n",
    "ax.set_xticks([0.1, 0.3, 0.5, 0.7])\n",
    "\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "f.savefig(\"3_Brevity_words.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretizacion de la variable tiempo para hacer un test de spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_words_filtered = lista_words[lista_words.duration<2]\n",
    "lista_words_filtered = lista_words_filtered.assign(bin_dur = lista_words_filtered.duration.round(0))\n",
    "brevitynull = lista_words_filtered.groupby(\"token\").median()[[\"bin_dur\", \"repetitions\"]]\n",
    "print(scipy.stats.spearmanr(brevitynull.bin_dur, np.log(brevitynull.repetitions)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Brevity law  Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla_datos_brev = tabla_datos.copy()\n",
    "#tabla_datos_brev = tabla_datos_brev.assign(numletterPhon=tabla_datos_brev[tabla_datos_brev.tipe==\"w\"].numletters/tabla_datos_brev[tabla_datos_brev.tipe==\"w\"].numphonemes)\n",
    "#tabla_datos_brev.fillna(method='ffill', inplace=True)\n",
    "\n",
    "lista_fonemabrev = tabla_datos_brev[((tabla_datos_brev.tipe == 'p') & (tabla_datos_brev.token != 'SIL') & (tabla_datos_brev.token != 'VOCNOISE') \n",
    "                            & (tabla_datos_brev.token != 'UNKNOWN') & (tabla_datos_brev.token != 'NOISE') & (tabla_datos_brev.token != 'LAUGH')\n",
    "                            & (tabla_datos_brev.token != 'IVER')\n",
    "                            & (tabla_datos_brev.token.str.endswith(\"}\") == False) & (tabla_datos_brev.token.str.startswith(\"{\") == False))]\n",
    "\n",
    "lista_fonemabrev = lista_fonemabrev.assign(repetitions=lista_fonemabrev.token.map(lista_fonemabrev.token.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brevityp = lista_fonemabrev.groupby(\"token\").median()[[\"duration\", \"repetitions\"]]\n",
    "brevityp = brevityp[brevityp.repetitions>50]\n",
    "brevityp2 = brevityp[brevityp.repetitions>50]\n",
    "\n",
    "print(len(brevityp))\n",
    "#The two-sided p-value for a hypothesis test whose null hypothesis is that two sets of data are uncorrelated, \n",
    "#A small p-value (typically â¤ 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis\n",
    "print(\"Frequencia phonemes vs duracion(segundos:\")\n",
    "print(scipy.stats.spearmanr(brevityp.repetitions, brevityp.duration))\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Frequencia phonemes vs duracion(segundos) si >50:\")\n",
    "print(scipy.stats.spearmanr(brevityp2.repetitions, brevityp2.duration))\n",
    "print()\n",
    "print()\n",
    "\n",
    "### AHORA VAMOS A HACER UN BINING\n",
    "freq, pos = binear_datos(brevityp.repetitions, brevityp.duration, bins=8, log = True)\n",
    "\n",
    "f, ax1 = plt.subplots()\n",
    "ax1.plot(brevityp.duration, brevityp.repetitions, 'o', lw = 1, ms = 5, zorder=2, alpha=0.6, color = 'lightgrey')\n",
    "ax1.plot(pos, freq, 's', lw = 2, ms = 11, label= 'phonemes', zorder=2, alpha=1, color = 'darkorange')\n",
    "\n",
    "ax1.set_xlabel(r\"$Median$\"  + \" \" + r\"$time$\"  + \" \" + r\"$ duration$\"  + \" \" + r\"$ (seconds)$\", fontsize=12)\n",
    "#ax.legend()\n",
    "ax1.set_yscale(\"log\")\n",
    "\n",
    "ax1.set_ylabel(\"Frequency\", fontsize=12)\n",
    "#ax1.legend(frameon=False)\n",
    "\n",
    "\n",
    "ax1.set_xticks([0.05, 0.09, 0.13])\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "def func_exp(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "def func_log(x, a, b): # FUNCION LOGARITMICA que utiliza los mismos parametros\n",
    "    return 1/(-b) * ( np.log(x) - np.log(a) )\n",
    "\n",
    "## FITING\n",
    "x = brevityp2.repetitions\n",
    "y = brevityp2.duration\n",
    "\n",
    "xplot = np.linspace(0.06, 0.12, 3)\n",
    "popt, pcov = curve_fit(func_log, x, y)\n",
    "ax1.plot(xplot, func_exp(xplot, *popt), \"--\", color = \"darkred\", lw = 2)\n",
    "\n",
    "ax1.text(0.6, 0.8, r'$\\lambda = $' + \"{:.{}f}\".format(popt[1], 0), horizontalalignment='center', fontsize = 15, verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "residuals = y - func_log(x, *popt)\n",
    "ss_res = np.sum(residuals**2)\n",
    "ss_tot = np.sum((y-np.mean(y))**2)\n",
    "r_squared = 1 - (ss_res / ss_tot)\n",
    "print(\"R2: \" + str(r_squared))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax1.tick_params(axis='x', labelsize=12)\n",
    "ax1.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f.savefig(\"3_Brevity_phonemes.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Heaps Law\n",
    "Para heap law, leemos secuencialmente las conversaciones para ver como aumenta el vocabulario en funcion del tiempo transcurrido o del numero de palabras transcurridas\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero aÃ±adimos una columna de zeros\n",
    "lista_words['first_time_token'] = 0 \n",
    "first_ocurrences =  lista_words.drop_duplicates(subset=[\"token\"]).index\n",
    "lista_words.loc[first_ocurrences, \"first_time_token\"] = 1\n",
    "\n",
    "\n",
    "# Creamos el fichero de tiempos\n",
    "total_time = 0\n",
    "lista_tiempo_transcurrido = []\n",
    "path = list(lista_words[0:1].path)[0]\n",
    "\n",
    "for index, row in lista_words.iterrows():\n",
    "    if path != row.path:\n",
    "        total_time += previoustime # Guarda\n",
    "        path = row.path\n",
    "    previoustime = row.tinit\n",
    "    lista_tiempo_transcurrido.append(total_time+row.tinit)\n",
    "\n",
    "xheapt = lista_tiempo_transcurrido\n",
    "yheapt = lista_words.first_time_token.cumsum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# decimate logaritmic\n",
    "decimation = np.logspace(0, np.log10(len(xheapt)-1),100, dtype=\"int\")\n",
    "decimation = np.asarray([0] + list(decimation))\n",
    "\n",
    "xheapt = np.asarray(xheapt) - xheapt[1] - 0.2\n",
    "xheaptdecimate = pd.DataFrame(xheapt).loc[decimation, :]\n",
    "xheaptdecimate = xheaptdecimate[0].values\n",
    "\n",
    "yheaptdecimate = pd.DataFrame(list(yheapt)).loc[decimation, :]\n",
    "yheaptdecimate = yheaptdecimate[0].values\n",
    "\n",
    "xheapL = pd.DataFrame(lista_words.numtoken.values).loc[decimation, :]\n",
    "xheapL = xheapL[0].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heaps corpus escrito\n",
    "xheapL = pd.DataFrame(lista_words.numtoken.values).loc[decimation, :]\n",
    "xheapL = xheapL[0].values\n",
    "\n",
    "yheapL = pd.DataFrame(list(yheapt)).loc[decimation, :]\n",
    "yheapL = yheapL[0].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FIGURA HEAPS\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "# MAIN PLOT\n",
    "lns1 = ax.plot(xheaptdecimate, yheaptdecimate -2 , 'o', lw = 2, ms = 8, label= 'V(L)', zorder=2, alpha=0.7, color=\"darkblue\")\n",
    "\n",
    "xline = np.array([0.6*10**3, 3*10**5])\n",
    "ax.text(0.82, 0.57, r'$\\beta = \\gamma = 0.63$', horizontalalignment='center', fontsize = 14, verticalalignment='center', transform=ax.transAxes)\n",
    "ax.plot(xline, np.power(2*10**0*xline, 0.63), \"--\", color = \"k\", lw=1)\n",
    "\n",
    "\n",
    "ax.set_xlim([10**-1, 10**6])\n",
    "\n",
    "ax.set_xlabel(r\"$Time$\" +\" \" + r\"$elapsed$\" +\" \" + r\"$T$\"+ \" \" + r\"$(seconds)$\", fontsize=14)\n",
    "ax.set_ylabel( r\"$Vocabulary$\" + \" \" +  r\"$V$\", fontsize=14)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.set_xlim([10**-1, 10**6])\n",
    "\n",
    "\n",
    "lns2 = ax2.plot(xheapL, yheaptdecimate, 'd', lw = 2, ms = 8, label= 'V(T)', zorder=5, alpha=0.7, color = \"darkolivegreen\")\n",
    "ax2.set_xlabel(r\"$Words$\" +\" \" + r\"$elapsed$\" + \" \" + r\"$L$\", fontsize=14)\n",
    "\n",
    "\n",
    "\n",
    "# added these three lines\n",
    "lns = lns1+lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns, labs, loc=2, frameon = False, fontsize = 14)\n",
    "\n",
    "\n",
    "ax.set_xticks([10**0,10**2, 10**4, 10**6])\n",
    "ax2.set_xticks([10**0,10**2, 10**4, 10**6])\n",
    "ax.set_yticks([10**0,10**2, 10**4])\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "ax2.tick_params(axis='x', labelsize=12)\n",
    "\n",
    "\n",
    "\n",
    "f.savefig(\"4_HeapLaw.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(4)\n",
    "lista_words[\"informant\"] = lista_words.path.str[:3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURA HEAPS RANDOM\n",
    "f, ax = plt.subplots()\n",
    "ax2 = ax.twiny()\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    groups = [df for _, df in lista_words.groupby(\"informant\")]\n",
    "    random.shuffle(groups)\n",
    "    lista_word_rand = pd.concat(groups).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # Primero aÃ±adimos una columna de zeros\n",
    "    lista_word_rand['first_time_token'] = 0 \n",
    "    first_ocurrences_rand =  lista_word_rand.drop_duplicates(subset=[\"token\"]).index\n",
    "    lista_word_rand.loc[first_ocurrences_rand, \"first_time_token\"] = 1\n",
    "\n",
    "\n",
    "    # Creamos el fichero de tiempos\n",
    "    total_time = 0\n",
    "    lista_tiempo_transcurrido = []\n",
    "    path = list(lista_word_rand[0:1].path)[0]\n",
    "\n",
    "    for index, row in lista_word_rand.iterrows():\n",
    "        if path != row.path:\n",
    "            total_time += previoustime # Guarda\n",
    "            path = row.path\n",
    "        previoustime = row.tinit\n",
    "        lista_tiempo_transcurrido.append(total_time+row.tinit)\n",
    "\n",
    "    xheapt_rand = lista_tiempo_transcurrido\n",
    "    yheapt_rand = lista_word_rand.first_time_token.cumsum()\n",
    "\n",
    "\n",
    "    # decimate logaritmic\n",
    "    decimation = np.logspace(0, np.log10(len(xheapt_rand)-1),100, dtype=\"int\")\n",
    "    decimation = np.asarray([0] + list(decimation))\n",
    "\n",
    "    xheapt_rand = np.asarray(xheapt_rand) - xheapt_rand[1] - 0.2\n",
    "    xheaptdecimate_rand = pd.DataFrame(xheapt_rand).loc[decimation, :]\n",
    "    xheaptdecimate_rand = xheaptdecimate_rand[0].values\n",
    "\n",
    "    yheaptdecimate_rand = pd.DataFrame(list(yheapt_rand)).loc[decimation, :]\n",
    "    yheaptdecimate_rand = yheaptdecimate_rand[0].values\n",
    "\n",
    "    xheapL_rand = pd.DataFrame(lista_word_rand.index.values + 1).loc[decimation, :]\n",
    "    xheapL_rand = xheapL_rand[0].values\n",
    "    \n",
    "    # Heaps corpus escrito\n",
    "    yheapL_rand = pd.DataFrame(list(yheapt_rand)).loc[decimation, :]\n",
    "    yheapL_rand = yheapL_rand[0].values\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # PLOT\n",
    "    if i == 0:\n",
    "        lns1 = ax.plot(xheaptdecimate_rand, yheaptdecimate_rand -2 , '-o', lw = 1, ms = 3, label= 'V(T)', zorder=2, alpha=0.6, color=\"darkblue\")\n",
    "        lns2 = ax2.plot(xheapL_rand, yheaptdecimate_rand, '-d', lw = 1, ms = 3, label= 'V(L)', zorder=5, alpha=0.6, color = \"darkolivegreen\")\n",
    "    # PLOT\n",
    "    if i != 0:\n",
    "        lns10 = ax.plot(xheaptdecimate_rand, yheaptdecimate_rand -2 , '-o', lw = 1, ms = 3, zorder=2, alpha=0.6, color=\"darkblue\")\n",
    "        lns20 = ax2.plot(xheapL_rand, yheaptdecimate_rand, '-d', lw = 1, ms = 3, zorder=5, alpha=0.6, color = \"darkolivegreen\")\n",
    "    \n",
    "    \n",
    "xline = np.array([0.6*10**3, 3*10**5])\n",
    "ax.text(0.82, 0.57, r'$\\beta = \\gamma = 0.63$', horizontalalignment='center', fontsize = 14, verticalalignment='center', transform=ax.transAxes)\n",
    "ax.plot(xline, np.power(2*10**0*xline, 0.63), \"--\", color = \"k\", lw=1)\n",
    "\n",
    "ax.set_xlim([10**-1, 10**6])\n",
    "\n",
    "ax.set_xlabel(r\"$Time$\" +\" \" + r\"$elapsed$\" +\" \" + r\"$T$\"+ \" \" + r\"$(seconds)$\", fontsize=14)\n",
    "ax.set_ylabel( r\"$Vocabulary$\" + \" \" +  r\"$V$\", fontsize=14)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.set_xlim([10**-1, 10**6])\n",
    "\n",
    "\n",
    "ax2.set_xlabel(r\"$Words$\" +\" \" + r\"$elapsed$\" + \" \" + r\"$L$\", fontsize=14)\n",
    "\n",
    "\n",
    "# added these three lines\n",
    "lns = lns1+lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns, labs, loc=2, frameon = False, fontsize = 14)\n",
    "\n",
    "\n",
    "ax.set_xticks([10**0,10**2, 10**4, 10**6])\n",
    "ax2.set_xticks([10**0,10**2, 10**4, 10**6])\n",
    "ax.set_yticks([10**0,10**2, 10**4])\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "ax2.tick_params(axis='x', labelsize=12)\n",
    "\n",
    "\n",
    "f.savefig(\"4_HeapLaw_RANDOM_order.pdf\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# FITTED DATA #######################################################\n",
    "####################################################################\n",
    "from scipy.optimize import curve_fit\n",
    "def func_powerlaw(x, m, c, c0):\n",
    "    return c0 + x**m * c\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "\n",
    "x = xheaptdecimate[50::]\n",
    "y = yheaptdecimate[50::]\n",
    "ax.plot(x, y, \"o\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "sol2 = curve_fit(func_powerlaw, x, y)\n",
    "print(sol2)\n",
    "expy = func_powerlaw(x, sol2[0][0], sol2[0][1], sol2[0][2])\n",
    "ax.plot(x, expy, \"--\", lw=2)\n",
    "print(\"exponente = \" + str(sol2[0][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Speech Rate\n",
    "Calcular el numero de palabras por minuto y como converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_transcurridas = lista_words.numtoken.values\n",
    "tiempo = np.asarray(lista_tiempo_transcurrido)\n",
    "tiempo_minutos = tiempo/60\n",
    "\n",
    "speech_rate = palabras_transcurridas/tiempo_minutos\n",
    "\n",
    "\n",
    "\n",
    "# FIGURA HEAPS\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "# MAIN PLOT\n",
    "# decimate logaritmic\n",
    "decimation = list(np.linspace(0, len(palabras_transcurridas)-1, 500, dtype=\"int\"))\n",
    "\n",
    "\n",
    "ax.plot(palabras_transcurridas[decimation], speech_rate[decimation] , '-', lw = 3, ms = 2, label= 'Speech Rate', zorder=2, alpha=0.7, color=\"black\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xlabel(r\"$Words$\" +\" \" + r\"$elapsed$\" + \" \" + r\"$L$\", fontsize=14)\n",
    "ax.set_ylabel(r\"$Speech$\" +\" \" + r\"$rate$\" + \" \" + r\"$L/T$\"+ \" \" + r\"$(\\frac{words}{minute})$\", fontsize=14)\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "\n",
    "f.savefig(\"SI_SpeechRate.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Menzerath Altmann law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Existen divesas posibles definiciones de Menzerath Altmann law. Por ello vamos a intentar varias y comaprarlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Sentences length (number of words) VS words length (number of letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentenceSize_numwords = []\n",
    "WordsSize_numletters = []\n",
    "WordsSize_numphonemes = []  # Para la definicion2\n",
    "WordsSize_seconds = []  # Para la definicion3\n",
    "\n",
    "\n",
    "\n",
    "for sentence in tabla_datos.sentence.unique():\n",
    "    tablaSentence = tabla_datos[tabla_datos.sentence == sentence]\n",
    "    numwords = len(tablaSentence[tablaSentence.tipe == \"w\"])\n",
    "    if numwords == 100:\n",
    "        stop\n",
    "    SentenceSize_numwords.append(numwords)\n",
    "\n",
    "    numletters_mean = np.mean(tablaSentence[tablaSentence.tipe == \"w\"].numletters)\n",
    "    WordsSize_numletters.append(numletters_mean)\n",
    "    \n",
    "    # Para la definicion2\n",
    "    numphonemes_mean = np.mean(tablaSentence[tablaSentence.tipe == \"w\"].numphonemes)\n",
    "    WordsSize_numphonemes.append(numphonemes_mean)\n",
    "    \n",
    "    # Para la definicion3\n",
    "    word_duration_mean = np.mean(tablaSentence[tablaSentence.tipe == \"w\"].duration)\n",
    "    WordsSize_seconds.append(word_duration_mean)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "def func_powerlawexp(x, alfa, beta, c):\n",
    "    return alfa * x**(beta) * np.exp(-c*x)\n",
    "\n",
    "# def func_powerlawexp(x, beta):\n",
    "#     #c = b/6\n",
    "#     #0.4 = a*e^(-b/6)\n",
    "    \n",
    "#     return (0.4/(np.exp(-beta/6)) * x**(-beta) * np.exp(-beta/6*x))\n",
    "\n",
    "numbins= 10\n",
    "\n",
    "for setsize in [41,80]:\n",
    "    for bin15 in [True, False]:\n",
    "    \n",
    "        # FIGURA MENZERATH\n",
    "        f, ax = plt.subplots()\n",
    "\n",
    "        ###########################################################################\n",
    "        # MENZERATH FRSES WORDS (SECONDS) #########################################\n",
    "        #########################################################################################################\n",
    "\n",
    "        Mentable_sec = pd.DataFrame({\"SentSize\":SentenceSize_numwords, \"WordsSize\": WordsSize_seconds})\n",
    "        Mentable_sec1 = Mentable_sec[ Mentable_sec[\"SentSize\"]<setsize]\n",
    "\n",
    "        azul = Mentable_sec1.groupby(\"SentSize\").mean()\n",
    "        pos0 = azul.index.values\n",
    "        freq0 = azul.WordsSize.values\n",
    "        pos015, freq015 = binear_datos(pos0, freq0, bins=numbins, log = False)\n",
    "\n",
    "\n",
    "        # MAIN PLOT\n",
    "        #ax.plot(Mentable_sec.SentSize.values, Mentable_sec.WordsSize.values , 'o', lw = 2, ms = 2, zorder=2, alpha=1, color = \"gray\")\n",
    "        ax.plot(Mentable_sec1.SentSize.values[0:3000], Mentable_sec1.WordsSize.values[0:3000], 'o', ms = 1, zorder=2, alpha=0.5, color = 'lightgrey', label = r\"$data$\")\n",
    "        if bin15 == False:\n",
    "            ax.plot(pos0, freq0, 'o', lw = 2, ms = 11, zorder=2, alpha=1, color = \"darkblue\", label = r\"$binned$\" + \" \" + r\"$data$\")\n",
    "        elif bin15 == True:\n",
    "            ax.plot(pos015, freq015, 'o', lw = 2, ms = 11, zorder=2, alpha=1, color = \"darkblue\", label = r\"$binned$\" + \" \" + r\"$data$\")\n",
    "\n",
    "        ax.set_xlabel(\"BG size in number of words\")\n",
    "        ax.set_ylabel(r\"$Word$\" + \" \" + r\"$\\overline{size}$\" + \" \" + r\"$(seconds)$\")\n",
    "\n",
    "        ax.set_ylim([0.0, 0.45])\n",
    "\n",
    "        ax.set_xticks([0, 20, 40, 60])\n",
    "        ax.set_yticks([0, 0.1, 0.2, 0.3, 0.4])\n",
    "\n",
    "\n",
    "        x = np.asarray(pos0)\n",
    "        y = np.asarray(freq0)\n",
    "\n",
    "\n",
    "        # POWER LAW EXPONENTIAL\n",
    "        print(\"Power law exponential seconds:\")\n",
    "        #popt, pcov = curve_fit(func_powerlawexp, x, y)\n",
    "        popt, pcov = curve_fit(func_powerlawexp, pos0, freq0)\n",
    "\n",
    "        expy = func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "        ax.plot(x, expy, \"--\", lw=2, color=\"red\", label = r\"$fit$\" + \" \" + r\"$to$\" + \" \" + r\"$y = ax^b e^{-cx}$\")\n",
    "\n",
    "        velocityx = x\n",
    "        velocityy = 1/expy\n",
    "\n",
    "        print(\"exponentes = \" + str(popt))\n",
    "\n",
    "        # GET R2\n",
    "        residuals = y - func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "        # residuals = y - func_powerlawexp(x, popt[0])\n",
    "\n",
    "        ss_res = np.sum(residuals**2)\n",
    "        ss_tot = np.sum((y - np.mean(y))**2)\n",
    "        R2 = 1 - (ss_res / ss_tot)\n",
    "        print(\"R2:\" + str(R2))\n",
    "        print(\"\")\n",
    "\n",
    "        ax.legend(frameon = False, loc = 4, fontsize=12)\n",
    "\n",
    "        #ax.set_xlim([0, 40])\n",
    "\n",
    "\n",
    "        ax.tick_params(axis='x', labelsize=12)\n",
    "        ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "        # NUM LETTERS ############################################################################\n",
    "        Mentableletters = pd.DataFrame({\"SentSize\":SentenceSize_numwords, \"WordsSize\": WordsSize_numletters})\n",
    "        Mentableletters1 = Mentableletters[ Mentableletters[\"SentSize\"]<setsize]\n",
    "        #Menzerathletters = Mentableletters.groupby(\"SentSize\").mean()\n",
    "\n",
    "        subax = plt.axes([ax.get_position().x0, ax.get_position().y0, .30, .25])\n",
    "\n",
    "        #pos1, freq1 = binear_datos(Mentableletters1.SentSize.values, Mentableletters1.WordsSize.values, bins=15, log = False)\n",
    "        azul = Mentableletters1.groupby(\"SentSize\").mean()\n",
    "        pos1 = azul.index.values\n",
    "        freq1 = azul.WordsSize.values\n",
    "        \n",
    "        pos115, freq115 = binear_datos(pos1, freq1, bins=numbins, log = False)\n",
    "\n",
    "\n",
    "\n",
    "        # MAIN PLOT\n",
    "        #subax.plot(Mentableletters.SentSize.values, Mentableletters.WordsSize.values , 'o', lw = 2, ms = 2, zorder=2, alpha=1, color = \"gray\")\n",
    "        #subax.plot(Menzerathletters, 'o', lw = 2, ms = 2, label= 'seconds', zorder=2, alpha=1, color = \"gray\")\n",
    "        subax.plot(Mentableletters.SentSize.values[0:3000], Mentableletters.WordsSize.values[0:3000], 'o', ms = 1, zorder=2, alpha=0.5, color = 'lightgrey')\n",
    "\n",
    "        \n",
    "        if bin15 == False:\n",
    "            subax.plot(pos1, freq1, 'o', lw = 2, ms = 6, zorder=2, alpha=1, color = \"darkblue\")\n",
    "        elif bin15 == True:\n",
    "            subax.plot(pos115, freq115, 'o', lw = 2, ms = 6, zorder=2, alpha=1, color = \"darkblue\")\n",
    "        \n",
    "        \n",
    "\n",
    "        #subax.set_xlabel(\"BG (words)\")\n",
    "        subax.set_ylabel( r\"$Word$\" + \" \" + r\"$\\overline{size}$\" + \"\\n\" + r\"$(chars)$\", rotation = 0)\n",
    "        subax.yaxis.set_label_coords(1.22, 0.65)\n",
    "\n",
    "\n",
    "        subax.yaxis.set_label_position(\"right\")\n",
    "        subax.yaxis.set_ticks_position(\"right\")\n",
    "\n",
    "        #subax2.set_ylabel(\"freq\",rotation=0)\n",
    "        subax.xaxis.set_label_position(\"top\")\n",
    "        subax.xaxis.set_ticks_position(\"top\")\n",
    "        subax.set_ylim([3.7, 4.2])\n",
    "\n",
    "        subax.set_yticks([3.8, 4.1])\n",
    "        subax.set_xticks([10, 30, 50])\n",
    "        #subax.set_xlim([0, 40])\n",
    "\n",
    "\n",
    "        x = np.asarray(pos1)[2::]\n",
    "        y = np.asarray(freq1)[2::]\n",
    "\n",
    "\n",
    "\n",
    "        subax.tick_params(axis='x', labelsize=12)\n",
    "        subax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "        # POWER LAW EXPONENTIAL\n",
    "        print(\"Power law exponential chars:\")\n",
    "        popt, pcov = curve_fit(func_powerlawexp, x, y)\n",
    "        expy = func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "        subax.plot(x, expy, \"--\", lw=2, color=\"red\")\n",
    "        print(\"exponentes = \" + str(popt))\n",
    "\n",
    "        # GET R2\n",
    "        residuals = y - func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "        ss_res = np.sum(residuals**2)\n",
    "        ss_tot = np.sum((y - np.mean(y))**2)\n",
    "        R2 = 1 - (ss_res / ss_tot)\n",
    "        print(\"R2:\" + str(R2))\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "        #################################################################################\n",
    "        # Sentences length (number of words) VS words length (number of phonemes)\n",
    "        #############################################################################\n",
    "\n",
    "        # Ploteamos\n",
    "        subax2 = plt.axes([ax.get_position().x1 - .30, ax.get_position().y1 - .25, .30, .25])\n",
    "\n",
    "        Mentable = pd.DataFrame({\"SentSize\":SentenceSize_numwords, \"WordsSize\": WordsSize_numphonemes})\n",
    "        Mentable1 = Mentable[Mentable.SentSize<setsize]\n",
    "        #Menzerathphonemes = Mentable.groupby(\"SentSize\").mean()\n",
    "\n",
    "\n",
    "        #pos2, freq2 = binear_datos(Mentable1.SentSize.values, Mentable1.WordsSize.values, bins=15, log = False)\n",
    "        azul = Mentable1.groupby(\"SentSize\").mean()\n",
    "        pos2 = azul.index.values\n",
    "        freq2 = azul.WordsSize.values\n",
    "        pos215, freq215 = binear_datos(pos2, freq2, bins=numbins, log = False)\n",
    "\n",
    "\n",
    "        # MAIN PLOT\n",
    "        #subax2.plot(Menzerath, 'o', lw = 2, ms = 2, zorder=2, alpha=1, color = \"gray\")\n",
    "        subax2.plot(Mentable.SentSize.values[0:3000], Mentable.WordsSize.values[0:3000], 'o', ms = 1, zorder=2, alpha=0.5, color = 'lightgrey')\n",
    "        if bin15 == False:\n",
    "            subax2.plot(pos2, freq2, 'o', lw = 2, ms = 6, zorder=2, alpha=1, color = \"darkblue\")\n",
    "        elif bin15 == True:\n",
    "            subax2.plot(pos215, freq215, 'o', lw = 2, ms = 6, zorder=2, alpha=1, color = \"darkblue\")\n",
    "        \n",
    "        \n",
    "        #subax2.set_xlabel(r\"$BG (words)$\")\n",
    "        subax2.set_ylabel(r\"$Word$\" + \" \" + r\"$\\overline{size}$\" + \"\\n\" + r\"$(phon)$\", rotation = 0)\n",
    "        subax2.yaxis.set_label_coords(-0.25, 0.35)\n",
    "\n",
    "        subax2.set_xticks([10, 30, 50])\n",
    "        subax2.set_yticks([2.8, 3.4])\n",
    "        subax2.set_ylim([2.6,3.5])\n",
    "        #subax2.set_xlim([0, 40])\n",
    "\n",
    "\n",
    "\n",
    "        subax2.tick_params(axis='x', labelsize=12)\n",
    "        subax2.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        x = np.asarray(pos2)[2::]\n",
    "        y = np.asarray(freq2)[2::]\n",
    "\n",
    "\n",
    "        # POWER LAW EXPONENTIAL\n",
    "        print(\"Power law exponential phonemes:\")\n",
    "        popt, pcov = curve_fit(func_powerlawexp, x, y)\n",
    "        expy = func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "        subax2.plot(x, expy, \"--\", lw=2, color=\"red\")\n",
    "        print(\"exponentes = \" + str(popt))\n",
    "\n",
    "        # GET R2\n",
    "        residuals = y - func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "        ss_res = np.sum(residuals**2)\n",
    "        ss_tot = np.sum((y - np.mean(y))**2)\n",
    "        R2 = 1 - (ss_res / ss_tot)\n",
    "        print(\"R2:\" + str(R2))\n",
    "        print(\"\")\n",
    "\n",
    "        if setsize == 80:\n",
    "            if bin15 == False:\n",
    "                f.savefig(\"5_Menzerath_BG_WORDS_allBG.pdf\")\n",
    "            elif bin15 == True:\n",
    "                f.savefig(\"5_Menzerath_BG_WORDS_allBG_bin15.pdf\")\n",
    "        elif setsize == 41:\n",
    "            subax.set_xticks([10, 30])\n",
    "            subax2.set_xticks([10, 30])\n",
    "            subax2.set_xlim([0, 42])\n",
    "            subax.set_xlim([0, 42])\n",
    "            \n",
    "            if bin15 == False:\n",
    "                f.savefig(\"5_Menzerath_BG_WORDS_less41.pdf\")\n",
    "            elif bin15 == True:\n",
    "                f.savefig(\"5_Menzerath_BG_WORDS_less41_bin15.pdf\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos para la figura Velocidad MAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos1, freq1 = binear_datos(Mentableletters1.SentSize.values, Mentableletters1.WordsSize.values, bins=15, log = False)\n",
    "\n",
    "\n",
    "\n",
    "speechVelocity = 1/freq015\n",
    "BGsize = pos015\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(BGsize, speechVelocity, 'o')\n",
    "ax.plot(velocityx, velocityy, '--')\n",
    "ax.set_ylim([2,6])\n",
    "ax.set_xlim([0,70])\n",
    "dfvelocity = pd.DataFrame({\"BGsize\":BGsize, \"Velocity\":speechVelocity})\n",
    "dfvelocity.to_csv(\"puntosFiguraVelocity.txt\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todos los posibles ajustes de MAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def func_powerlawexp(x, alfa, beta, c):\n",
    "    return alfa * x**(beta) * np.exp(-c*x)\n",
    "\n",
    "\n",
    "def adjust_powerlawexp(xdata, ydata):\n",
    "    popt, pcov = curve_fit(func_powerlawexp, xdata, ydata)\n",
    "    expy = func_powerlawexp(xdata, popt[0], popt[1], popt[2])\n",
    "\n",
    "    # GET R2\n",
    "    residuals = ydata - func_powerlawexp(xdata, popt[0], popt[1], popt[2])\n",
    "    # residuals = y - func_powerlawexp(x, popt[0])\n",
    "\n",
    "    ss_res = np.sum(residuals**2)\n",
    "    ss_tot = np.sum((ydata - np.mean(ydata))**2)\n",
    "    R2 = 1 - (ss_res / ss_tot)\n",
    "    print(\"a|b|c| b/c = \" + str(popt[0].round(2)) + \" | \" + str(popt[1].round(4)) +\" | \"+  str(popt[2].round(5))+\" | \"+  str((popt[1]/popt[2]).round()) + \"  R2: \" + str(R2.round(2)))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "# ALL DATA BIN AZUL\n",
    "print(\"###### ALL DATA AZUL ############################\")\n",
    "azul = Mentable_sec.groupby(\"SentSize\").mean()\n",
    "print(\"BG seconds  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "Mentable1 = Mentable[Mentable.SentSize>2]\n",
    "azul = Mentable1.groupby(\"SentSize\").mean()\n",
    "print(\"BG>2 phonemes  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "azul = Mentable.groupby(\"SentSize\").mean()\n",
    "print(\"BG phonemes  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "Mentable1 = Mentableletters[Mentableletters.SentSize>2]\n",
    "azul = Mentable1.groupby(\"SentSize\").mean()\n",
    "print(\"BG>2 letters  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "azul = Mentableletters.groupby(\"SentSize\").mean()\n",
    "print(\"BG letters  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ALL DATA BIN AZUL\n",
    "print(\"###### DATA AZUL < 40 ############################\")\n",
    "\n",
    "Mentable_sec1 = Mentable_sec[Mentable_sec.SentSize<40]\n",
    "azul = Mentable_sec1.groupby(\"SentSize\").mean()\n",
    "print(\"BG seconds  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "\n",
    "Mentable1 = Mentable[Mentable.SentSize<40]\n",
    "Mentable1 = Mentable1[Mentable1.SentSize>2]\n",
    "azul = Mentable1.groupby(\"SentSize\").mean()\n",
    "print(\"BG>2 phonees  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "Mentable1 = Mentable[Mentable.SentSize<40]\n",
    "azul = Mentable1.groupby(\"SentSize\").mean()\n",
    "print(\"BG phonees  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "\n",
    "\n",
    "Mentableletters1 = Mentableletters[Mentableletters.SentSize<40]\n",
    "Mentableletters1 = Mentableletters1[Mentableletters1.SentSize>2]\n",
    "azul = Mentableletters1.groupby(\"SentSize\").mean()\n",
    "print(\"BG>2 letters  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "\n",
    "Mentableletters1 = Mentableletters[Mentableletters.SentSize<40]\n",
    "azul = Mentableletters1.groupby(\"SentSize\").mean()\n",
    "print(\"BG letters  \")\n",
    "adjust_powerlawexp(azul.index.values, azul.WordsSize.values)\n",
    "\n",
    "\n",
    "# ALL DATA GRIS\n",
    "print(\"###### ALL DATA GRIS ############################\")\n",
    "print(\"BG seconds  \")\n",
    "adjust_powerlawexp(Mentable_sec.SentSize.values, Mentable_sec.WordsSize.values)\n",
    "\n",
    "print(\"BG >2 phonemes  \")\n",
    "Mentable1 = Mentable[Mentable.SentSize>2]\n",
    "adjust_powerlawexp(Mentable1.SentSize.values, Mentable1.WordsSize.values)\n",
    "\n",
    "print(\"BG phonemes  \")\n",
    "adjust_powerlawexp(Mentable.SentSize.values, Mentable.WordsSize.values)\n",
    "\n",
    "print(\"BG letters >2 \")\n",
    "Mentable1 = Mentableletters[Mentableletters.SentSize>2]\n",
    "adjust_powerlawexp(Mentable1.SentSize.values, Mentable1.WordsSize.values)\n",
    "\n",
    "print(\"BG letters  \")\n",
    "adjust_powerlawexp(Mentableletters.SentSize.values, Mentableletters.WordsSize.values)\n",
    "\n",
    "\n",
    "# DATA < 40 GRIS\n",
    "print(\"###### DATA GRIS < 40 ############################\")\n",
    "print(\"BG seconds  \")\n",
    "Mentable_sec1 = Mentable_sec[Mentable_sec.SentSize<40]\n",
    "adjust_powerlawexp(Mentable_sec1.SentSize.values, Mentable_sec1.WordsSize.values)\n",
    "\n",
    "print(\"BG>2 phonemes  \")\n",
    "Mentable1 = Mentable[Mentable.SentSize<40]\n",
    "Mentable1 = Mentable1[Mentable1.SentSize>2]\n",
    "adjust_powerlawexp(Mentable1.SentSize.values, Mentable1.WordsSize.values)\n",
    "\n",
    "print(\"BG phonemes  \")\n",
    "Mentable1 = Mentable[Mentable.SentSize<40]\n",
    "adjust_powerlawexp(Mentable1.SentSize.values, Mentable1.WordsSize.values)\n",
    "\n",
    "print(\"BG >2 letters  \")\n",
    "Mentableletters1 = Mentableletters[Mentableletters.SentSize<40]\n",
    "Mentableletters1 = Mentableletters1[Mentableletters1.SentSize>2]\n",
    "adjust_powerlawexp(Mentableletters1.SentSize.values, Mentableletters1.WordsSize.values)\n",
    "\n",
    "\n",
    "print(\"BG letters  \")\n",
    "Mentableletters1 = Mentableletters[Mentableletters.SentSize<40]\n",
    "adjust_powerlawexp(Mentableletters1.SentSize.values, Mentableletters1.WordsSize.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo MAL equacion 9\n",
    "Modelo estocastico de la ecuacion 9 del paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azul = Mentable_sec1.groupby(\"SentSize\").mean()\n",
    "pos0 = azul.index.values\n",
    "freq0 = azul.WordsSize.values\n",
    "t1 = freq0[0]\n",
    "a = 0.364\n",
    "b = -0.227\n",
    "c = -0.0067\n",
    "k1 = t1/a\n",
    "k2 = b+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Menzerath-Altmann-Bartolo-Lucas\n",
    "Calcular para cada BG $log\\frac{t(n)}{t(n-1)}$ donde n es la es la palabra en la posicicion n. t(n) es el tiempo transcurrido hasta esa palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listat1 = lista_words.groupby(\"sentence\").first().duration.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k1 = np.exp(-c)\n",
    "k2 = b +1.3\n",
    "# Numero de palabras por BG real\n",
    "W_n = np.asarray(lista_words.groupby(\"sentence\").size())\n",
    "\n",
    "# Words durations\n",
    "words_durations = lista_words.duration.values[(lista_words.duration.values<5) & (lista_words.duration.values>0) ]\n",
    "\n",
    "lista_tn = []\n",
    "for i in range(100000):\n",
    "#     position = np.random.choice(np.arange(1, len(W_n)), 1)\n",
    "#     BG_size = W_n[position]\n",
    "#     tn = listat1[position]\n",
    "\n",
    "    BG_size = np.random.choice(W_n, 1) # Tamano de la frase\n",
    "    tn = t1\n",
    "    tn = np.random.choice(words_durations)\n",
    "    for j in range(2, int(BG_size)+1):\n",
    "        tn = tn*k1*( (1 + 1/(j-1))**k2 )\n",
    "#         tn = tn*k1*( (1 + k2/(j-1)) )\n",
    "    tn = tn + np.random.normal(0.14, 0.08)\n",
    "\n",
    "    lista_tn.append(tn)\n",
    "\n",
    "############################################################\n",
    "# REAL\n",
    "bin_mean_x_sentence, _, counts_norm_sentence, _ = histograma(dur_sentences, numero_bins=30)\n",
    "# ACTUAL\n",
    "lista_tn = np.asarray(lista_tn)\n",
    "lista_tn = lista_tn[lista_tn>0]\n",
    "bin_mean_x_sentence_Stocastic, _, counts_norm_sentence_Stocastic, _ = histograma(lista_tn, numero_bins=30)\n",
    "\n",
    "# Ploteamos\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(bin_mean_x_sentence, counts_norm_sentence, 'd', lw = 2, ms = 12, label= 'data', zorder=3, c=\"g\")\n",
    "ax.plot(bin_mean_x_sentence_Stocastic, counts_norm_sentence_Stocastic, '-o', lw = 1, ms = 1, label= 'theory based on MAL', zorder=3, c=\"k\")\n",
    "ax.legend(frameon = False)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlim([0.01, 30])\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"t (seconds)\", fontsize=14)\n",
    "ax.set_ylabel(\"P(t)\", fontsize=14)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "f.savefig(\"SI_MAL_model_BGlength_noise.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "k1 = np.exp(-c) - 0.1\n",
    "k2 = b+1.9\n",
    "# Numero de palabras por BG real\n",
    "W_n = np.asarray(lista_words.groupby(\"sentence\").size())\n",
    "# Words durations\n",
    "words_durations = lista_words.duration.values[(lista_words.duration.values<5) & (lista_words.duration.values>0) ]\n",
    "\n",
    "lista_tn = []\n",
    "g = lista_words.groupby(\"sentence\").size()\n",
    "\n",
    "def func_parl(i):\n",
    "    position = np.random.choice(np.arange(1, len(W_n)), 1)\n",
    "    BG_size = W_n[position] # Se toma un tamano de BG al azar\n",
    "    \n",
    "    \n",
    "    groups = g[g==int(BG_size)] # Se hace un subset de los BG reales con ese tamano\n",
    "    subset_df = lista_words[lista_words.sentence.isin(list(groups.index))]  # Se hace un subset de los BG reales con ese tamano\n",
    "    t1_n = subset_df.groupby(\"sentence\").first().duration.values # Se hace una lista de la primera palabra de BG con n palabras\n",
    "    tn = np.random.choice(t1_n, 1) # random de esa lista \n",
    "#     tn = listat1[position]\n",
    "\n",
    "    #BG_size = np.random.choice(W_n, 1) # Tamano de la frase\n",
    "    #tn = t1\n",
    "    #tn = np.random.choice(words_durations)\n",
    "    for j in range(2, int(BG_size)+1):\n",
    "#         tn = tn*k1*( (1 + 1/(j-1))**k2 )\n",
    "        tn = tn*k1*( (1 + k2/(j-1)) )\n",
    "#     tn = tn + np.random.normal(0.14, 0.08)\n",
    "    return float(tn)\n",
    "    #lista_tn.append(tn)\n",
    "\n",
    "pool = Pool(processes=7)\n",
    "lista_tn = pool.map(func_parl, range(100000))\n",
    "\n",
    "    \n",
    "############################################################\n",
    "# REAL\n",
    "bin_mean_x_sentence, _, counts_norm_sentence, _ = histograma(dur_sentences, numero_bins=30)\n",
    "# ACTUAL\n",
    "lista_tn = np.asarray(lista_tn)\n",
    "lista_tn = lista_tn[lista_tn>0]\n",
    "bin_mean_x_sentence_Stocastic, _, counts_norm_sentence_Stocastic, _ = histograma(lista_tn, numero_bins=30)\n",
    "\n",
    "# Ploteamos\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(bin_mean_x_sentence, counts_norm_sentence, 'd', lw = 2, ms = 12, label= 'data', zorder=3, c=\"g\")\n",
    "ax.plot(bin_mean_x_sentence_Stocastic, counts_norm_sentence_Stocastic, '-o', lw = 1, ms = 1, label= 'theory based on MAL', zorder=3, c=\"k\")\n",
    "ax.legend(frameon = False)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlim([0.01, 30])\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"t (seconds)\", fontsize=14)\n",
    "ax.set_ylabel(\"P(t)\", fontsize=14)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "f.savefig(\"SI_MAL_model_BGlength_t1n.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_t1 = lista_words.groupby(\"sentence\").nth(1).duration.values\n",
    "bin_mean_x_t1, _, counts_norm_t1, _ = histograma(words_t1, numero_bins=30)\n",
    "\n",
    "words_t2 = lista_words.groupby(\"sentence\").nth(5).duration.values\n",
    "bin_mean_x_t2, _, counts_norm_t2, _ = histograma(words_t2, numero_bins=30)\n",
    "\n",
    "\n",
    "words_t3 = lista_words.groupby(\"sentence\").nth(6).duration.values\n",
    "bin_mean_x_t3, _, counts_norm_t3, _ = histograma(words_t3, numero_bins=30)\n",
    "\n",
    "\n",
    "words_t4 = lista_words.groupby(\"sentence\").nth(7).duration.values\n",
    "bin_mean_x_t4, _, counts_norm_t4, _ = histograma(words_t4, numero_bins=30)\n",
    "\n",
    "\n",
    "# Ploteamos\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(bin_mean_x_t1, counts_norm_t1, '-o', lw = 1, ms = 3, label= 'words t1', zorder=3, c=\"k\")\n",
    "ax.plot(bin_mean_x_t2, counts_norm_t2, '-o', lw = 1, ms = 3, label= 'words t5', zorder=3, c=\"b\")\n",
    "ax.plot(bin_mean_x_t3, counts_norm_t3, '-o', lw = 1, ms = 3, label= 'words t6', zorder=3, c=\"g\")\n",
    "ax.plot(bin_mean_x_t4, counts_norm_t4, '-o', lw = 1, ms = 3, label= 'words t7', zorder=3, c=\"r\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "# ax.set_xscale(\"log\")\n",
    "ax.set_xlim([0.01, 0.8])\n",
    "# ax.set_yscale(\"log\")\n",
    "\n",
    "f.savefig(\"SI_distribution_wordlength_position_f.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 Words length (number of phonemes) VS phonemes length (seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WordLength_num_phonemes = tabla_datos[tabla_datos.tipe == \"w\"].numphonemes\n",
    "WordLength_num_phonemes = tabla_datos[tabla_datos.tipe == \"p\"].groupby(\"numtoken\").count()[\"token\"]\n",
    "PhonemeLength_seconds = tabla_datos[tabla_datos.tipe == \"p\"].groupby(\"numtoken\").mean().duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_powerlawexp(x, alfa, beta, gamma):\n",
    "    return alfa * x**(beta) * np.exp(-gamma*x)\n",
    "\n",
    "Mentable = pd.DataFrame({\"WordSize\":WordLength_num_phonemes.values, \"PhoneSize\": PhonemeLength_seconds.values})\n",
    "Menzerath = Mentable.groupby(\"WordSize\").mean()\n",
    "\n",
    "\n",
    "# FIGURA MENZERATH\n",
    "f, ax = plt.subplots()\n",
    "pos0, freq0 = binear_datos(Mentable.WordSize.values, Mentable.PhoneSize.values, bins=16, log = False)\n",
    "\n",
    "# MAIN PLOT\n",
    "ax.plot(Mentable.WordSize.values[0:1000], Mentable.PhoneSize.values[0:1000], 'o', ms = 1, zorder=2, alpha=0.5, label = r\"$data$\", color = 'lightgrey')\n",
    "ax.plot(Menzerath, 's', lw = 2, ms = 12, zorder=2, alpha=0.8, color=\"darkorange\", label=r\"$binned$\" + \" \" + r\"$data$\")\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xlabel(\"Word length(<phonem>)\",fontsize=12)\n",
    "ax.set_ylabel(\"Phoneme length (<sec>)\", fontsize=12)\n",
    "\n",
    "#ax.set_xscale(\"log\")\n",
    "#ax.set_yscale(\"log\")\n",
    "\n",
    "\n",
    "ax.set_xticks([2, 6, 10, 14, 18])\n",
    "ax.set_yticks([0.05, 0.08, 0.11])\n",
    "ax.set_ylim([0.03,0.14])\n",
    "\n",
    "x = pos0\n",
    "y = freq0\n",
    "\n",
    "# POWER LAW EXPONENTIAL\n",
    "print(\"Power law exponential:\")\n",
    "popt, pcov = curve_fit(func_powerlawexp, x, y)\n",
    "expy = func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "ax.plot(x, expy, \"--\", lw=2, color=\"red\", label = r\"$fit$\" + \" \" + r\"$to$\" + \" \" + r\"$y = ax^b e^{-cx}$\")\n",
    "print(\"exponentes = \" + str(popt))\n",
    "\n",
    "# GET R2\n",
    "residuals = y - func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "ss_res = np.sum(residuals**2)\n",
    "ss_tot = np.sum((y - np.mean(y))**2)\n",
    "R2 = 1 - (ss_res / ss_tot)\n",
    "print(\"R2:\" + str(R2))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "ax.legend(frameon = False, loc = \"best\", fontsize=12)\n",
    "\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "\n",
    "ax.set_xlabel(r\"$Word$\" + \" \" + r\"$\\overline{size}$\" + \" \" + r\"$in$\" + \" \" + r\"$number$\" + \" \" + r\"$of$\" + \" \" + r\"$phonemes$\")\n",
    "ax.set_ylabel(r\"$Phoneme$\" + \" \" + r\"$\\overline{size}$\" + \" \" + r\"$(seconds)$\")\n",
    "\n",
    "\n",
    "f.savefig(\"5_Menzerath_Words_Phonemes.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting version of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def func_powerlawexp(x, alfa, beta, gamma):\n",
    "    return alfa * x**(beta) * np.exp(-gamma*x)\n",
    "\n",
    "\n",
    "# RANDOM LN PHONEME GENERATION\n",
    "randLNphonemes = np.random.lognormal(mean=-2.681, sigma=0.59, size=10000)\n",
    "\n",
    "\n",
    "# NUMERO DE PHONEMAS POR PALABRA\n",
    "PhonemesPerWord = np.asarray(lista_words.numphonemes)\n",
    "RandPhonPerWord = np.random.choice(PhonemesPerWord, size = 10000, replace = True)\n",
    "\n",
    "\n",
    "randMeanSizePhon = []\n",
    "randTotalSize = []\n",
    "for i in RandPhonPerWord:\n",
    "    p = randLNphonemes**(-0.5*i)\n",
    "    #p = randLNphonemes**0\n",
    "    p = p/p.sum()\n",
    "    words = np.random.choice(randLNphonemes, size=i, p=p)\n",
    "    randMeanSizePhon.append(words.mean())\n",
    "    randTotalSize.append(words.sum())\n",
    "    \n",
    "\n",
    "    \n",
    "MentableRANDOM = pd.DataFrame({\"WordSize\":RandPhonPerWord, \"PhoneSize\": randMeanSizePhon})\n",
    "\n",
    "MenzerathRANDOM = MentableRANDOM.groupby(\"WordSize\").median()\n",
    "\n",
    "\n",
    "# FIGURA MENZERATH\n",
    "f, ax = plt.subplots()\n",
    "pos0, freq0 = binear_datos(MentableRANDOM.WordSize.values, MentableRANDOM.PhoneSize.values, bins=16, log = False)\n",
    "\n",
    "\n",
    "# MAIN PLOT\n",
    "ax.plot(MentableRANDOM.WordSize.values, MentableRANDOM.PhoneSize.values, 'o', ms = 1, zorder=2, alpha=0.5, color = 'lightgrey')\n",
    "ax.plot(MenzerathRANDOM, 's', lw = 2, ms = 12, zorder=2, alpha=0.8, color=\"darkorange\")\n",
    "#ax.set_ylim([0.0, 0.09])\n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################################3\n",
    "\n",
    "x = pos0\n",
    "y = freq0\n",
    "\n",
    "# POWER LAW EXPONENTIAL\n",
    "print(\"Power law exponential:\")\n",
    "popt, pcov = curve_fit(func_powerlawexp, x, y)\n",
    "expy = func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "ax.plot(x, expy, \"--\", lw=2, color=\"red\")\n",
    "print(\"exponentes = \" + str(popt))\n",
    "\n",
    "# GET R2\n",
    "residuals = y - func_powerlawexp(x, popt[0], popt[1], popt[2])\n",
    "ss_res = np.sum(residuals**2)\n",
    "ss_tot = np.sum((y - np.mean(y))**2)\n",
    "R2 = 1 - (ss_res / ss_tot)\n",
    "print(\"R2:\" + str(R2))\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Size-rank brevity law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meanDuration = lista_words.groupby(\"token\").duration.median()\n",
    "freq = lista_words.groupby(\"token\").repetitions.mean()\n",
    "df = pd.DataFrame({\"dur\":meanDuration, \"f\":freq })\n",
    "df = df.sort_values(\"f\", ascending=False)\n",
    "df[\"r\"] = np.arange(1,len(df)+1)\n",
    "\n",
    "dfRsup50 = df[df.r>0].copy()\n",
    "\n",
    "df[\"cumdur_rangos\"] = df.dur.cumsum()\n",
    "dfRsup50[\"cumdur_rangos\"] = dfRsup50.dur.cumsum()\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# PLOT ####################################################\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "meanx, meany = binear_datos(dfRsup50.r, dfRsup50.dur, bins=16, log = True)\n",
    "ax.plot(dfRsup50.r, dfRsup50.dur, \"o\", color = \"lightgray\", alpha = 0.8, ms=1, label =r\"$data$\")\n",
    "\n",
    "ax.plot(meanx, meany, \"o\", color = \"darkblue\", ms=11, alpha = 0.8, label =r\"$binned \\  data$\")\n",
    "\n",
    "it = 6\n",
    "\n",
    "\n",
    "# Primera pendiente\n",
    "xplot = dfRsup50.r\n",
    "yplot = 0.03*np.log(meanx[0:it+1])\n",
    "yplot = yplot - yplot[0] + meany[0]\n",
    "\n",
    "ax.plot(meanx[0:it+1], yplot - 0.06, \"-\", color=\"k\", lw=2, label = r\"$Eq. \\ 7$\") #label =r\"$\\hat{\\ell}_1 = \\frac{\\alpha _1}{\\lambda} \\cdot \\log(r _i) + K_1$\")\n",
    "\n",
    "\n",
    "# Segunda pendiente\n",
    "xplot = dfRsup50.r\n",
    "yplot = 0.07*np.log(meanx[it::])\n",
    "yplot = yplot - yplot[0] + meany[0]\n",
    "\n",
    "ax.plot(meanx[it::], yplot+0.06, \"-\", color=\"k\", lw=2) #label =r\"$\\hat{\\ell}_2 = \\frac{\\alpha _2}{\\lambda} \\cdot \\log(r _i) + K_2$\")\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "ax.set_xlabel(\"Rank r \", fontsize=14)\n",
    "ax.set_ylabel(r\"$\\hat{\\ell} $\", fontsize=14)\n",
    "\n",
    "ax.set_yticks([0.2, 0.5, 0.8])\n",
    "\n",
    "ax.set_xlim([0.8, 10000])\n",
    "\n",
    "ax.set_ylim([0.01, 1])\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "ax.legend(frameon=False, fontsize=14)\n",
    "f.savefig(\"SI_RANK_DURATION.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOISE IN SCATTER PLOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scatter plot sintetico entre dos variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 10000)\n",
    "y = 0.1 + 1000*np.exp(-8*x)\n",
    "\n",
    "\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(x, y, 'o', color=\"k\", ms=5)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\", rotation=0)\n",
    "ax.set_title(\"Original Function \" + \"$Y \\sim e^{\\lambda x}$\")\n",
    "f.savefig(\"SI_7_Scatter1.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Se aÃ±ade ruido sobre una de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 3, len(x))\n",
    "\n",
    "xruido = x + x*0.17*noise\n",
    "\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(xruido, y, \"o\", color=\"k\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\", rotation=0)\n",
    "ax.set_title(\"Original Data with noise in variable X\")\n",
    "f.savefig(\"SI_8_Scatter2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Bineamos sobre uno de los ejes para intentar eliminar el ruido\n",
    "En este caso no conseguimos recuperar la funcion original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posx, freqy = binear_datos(xruido, y, bins=40, log = False)\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "ax.plot(posx, freqy, \"o\", color=\"k\", label=\"Data after binning in X\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.plot(x, y, \"--\", lw=2, label=\"Original Function\")\n",
    "ax.legend(frameon = False)\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\", rotation=0)\n",
    "ax.set_title(\"Noise in variable X\")\n",
    "f.savefig(\"SI_9_Scatter3.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bineamos sobre el otro eje.\n",
    "En este caso recuperamos la funcion original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bin_y, bin_x  = binear_datos(y, xruido, log = True, bins =33)\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "ax.plot(bin_x, bin_y, \"o\", color=\"k\", label=\"Data after binning in Y\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.plot(x, y, \"--\", lw=2, label = \"Original Function\")\n",
    "ax.legend(frameon = False)\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\", rotation=0)\n",
    "f.savefig(\"SI_10_Scatter4.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
